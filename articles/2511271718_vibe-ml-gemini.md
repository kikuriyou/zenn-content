---
title: "Gemini CLI ã¨ Gemini 3.0 Pro ã§ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ‡ãƒ¼ã‚¿ã® Vibe Modeling (ã®ã‚ˆã†ãªä½•ã‹) ã‚’è©¦ã™"
emoji: "ðŸ€"
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢
topics: ["googlecloud", "vertexai", "gemini"]
published: true
---

[JP_Google Developer Experts Advent Calendar 2025](https://adventar.org/calendars/11658) ã® 2 æ—¥ç›®ã®è¨˜äº‹ã§ã™ã€‚

## ã¯ã˜ã‚ã«

æœ€è¿‘ã€ç”ŸæˆAIã‚’æ´»ç”¨ã—ãŸé–‹ç™ºæ‰‹æ³•ãŒå„æ‰€ã§æ´»ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚ãã®ä¸­ã§ã‚‚ç‰¹ã«è‡ªç„¶è¨€èªžã§æŒ‡ç¤ºã‚’å‡ºã™ã ã‘ã€å ´åˆã«ã‚ˆã£ã¦ã¯æ‰‹æ”¾ã—é‹è»¢çŠ¶æ…‹ã§æ§‹ç¯‰ã™ã‚‹ã€ã„ã‚ã‚†ã‚‹ Vibe Coding ã‚‚è©¦ã•ã‚Œã¦ã„ã¾ã™ã€‚

ç­†è€…ã®æ¥­å‹™ã§ã¯ã€ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ãŸæ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã‚„æŽ¨è«–ã‚’è¡Œã†æ©Ÿä¼šãŒå°‘ãªãã‚ã‚Šã¾ã›ã‚“ã€‚ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã”ã¨ã«ãƒ‡ãƒ¼ã‚¿ã¯ç•°ãªã‚Šã¾ã™ãŒã€å‰å‡¦ç†ã‚„ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æ§‹ç¯‰ãªã©ã€æ¯Žå›žä¼¼ãŸã‚ˆã†ãªã‚³ãƒ¼ãƒ‰ã‚’æ›¸ã„ã¦ã„ã‚‹ãªã¨æ„Ÿã˜ãŸã‚Šã€ã€Œã“ã®ã‚ãŸã‚Šã®ã‚«ã‚¹ã‚¿ãƒžã‚¤ã‚ºã‚’ç”ŸæˆAIã«ä»»ã›ã¦ã‚‚ã£ã¨åŠ¹çŽ‡åŒ–ã§ããªã„ï¼Ÿ Vibe Modeling ã§ããªã„ï¼Ÿã€ã¨æ€ã£ãŸã‚Šã—ã¾ã™ã€‚

ä¸€èˆ¬çš„ã«ã€ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢é–‹ç™ºã«ãŠã‘ã‚‹ç”ŸæˆAIæ”¯æ´ã§ã¯æŒ‡ç¤ºã‚’å…·ä½“çš„ã«ã™ã‚Œã°ã™ã‚‹ã»ã©æˆæžœç‰©ã®å“è³ªãŒé«˜ã¾ã‚‹å‚¾å‘ã«ã‚ã‚Šã¾ã™ã€‚ã“ã®è¨˜äº‹ã§ã¯ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æŒ‡ç¤ºãƒ¬ãƒ™ãƒ«ã‚’å¤‰åŒ–ã•ã›ã¦æ©Ÿæ¢°å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ã‚³ãƒ¼ãƒ‰ã‚’ç”Ÿæˆã—ï¼ˆæœ¬è¨˜äº‹ã§ã¯ã“ã‚Œã‚’å‹æ‰‹ã« Vibe Modeling ã¨å‘¼ã³ã¾ã™ï¼‰ã€ç”Ÿæˆã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰ãŒã©ã®ã‚ˆã†ã«å¤‰åŒ–ã™ã‚‹ã‹ã€ãã®å“è³ªã‚’ç¢ºèªã—ãŸã„ã¨æ€ã„ã¾ã™ã€‚

ãªãŠã€æœ¬è¨˜äº‹ã§ã¯ç”Ÿæˆã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰ã«å¯¾ã—ã¦å¯¾è©±ã—ãªãŒã‚‰ä¿®æ­£ã™ã‚‹ã‚ˆã†ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯å–ã‚‰ãšã€æœ€åˆã®æŒ‡ç¤ºã«ã‚ˆã‚‹ä¸€ç™ºå‡ºã—ã‚„ LLM ã®è‡ªå¾‹çš„ãªè©¦è¡ŒéŒ¯èª¤ã§ã©ã“ã¾ã§ã„ã‘ã‚‹ã‹ã‚’å®Ÿé¨“ã—ã¾ã™ã€‚

![](/images/articles/vibe-ml-gemini/vibe_modeling.jpg)
*Vibe Modeling ã®ã‚¤ãƒ¡ãƒ¼ã‚¸(æœ¬å½“ã‹?)*

## å®Ÿé¨“ã®è¨­å®š

æœ¬è¨˜äº‹ã§ä½¿ã£ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ [Online Retail II UCI](https://www.kaggle.com/datasets/mashlyn/online-retail-ii-uci) ã§ã€ãƒ‡ãƒ¼ã‚¿å†’é ­ã¯ã“ã®ã‚ˆã†ãªå†…å®¹ã«ãªã£ã¦ã„ã¾ã™ã€‚äº‹å‰ã«ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèªã—ãŸä¸Šã§ä»¥é™ã®ã‚³ãƒ¼ãƒ‰ç”Ÿæˆã‚’è©¦ã—ã¦ã„ã¾ã™ã€‚

![](/images/articles/vibe-ml-gemini/data.png)
*å…¥åŠ›ãƒ‡ãƒ¼ã‚¿*

ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ”¯æ´ã®ãƒ„ãƒ¼ãƒ«ã¨ã—ã¦ã¯ [Gemini CLI](https://github.com/google-gemini/gemini-cli) ã‚’ä½¿ç”¨ã—ã€LLM ã«ã¯ [Gemini 3.0 Pro](https://deepmind.google/models/gemini/pro/) ã‚’è¨­å®šã—ã¦ã„ã¾ã™ã€‚ãªãŠã€å®Ÿè¡Œãƒ­ã‚°ã‚’ç¢ºèªã—ãŸã¨ã“ã‚ã€ä¸€éƒ¨ã®å‡¦ç†ã§ã¯è‡ªå‹•çš„ã« Gemini 2.5 Flash ã«åˆ‡ã‚Šæ›¿ã‚ã£ã¦å‡¦ç†ã•ã‚Œã¦ã„ã‚‹å ´é¢ã‚‚ã‚ã‚Šã¾ã—ãŸã€‚
ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æŒ‡ç¤ºãƒ¬ãƒ™ãƒ«ã¨ã—ã¦ã¯æ¬¡ã® 3 æ®µéšŽã§å¤‰åŒ–ã•ã›ã¦ã€ç”Ÿæˆã•ã‚Œã‚‹ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ»æŽ¨è«–ã‚³ãƒ¼ãƒ‰ãŒã©ã®ã‚ˆã†ã«å¤‰åŒ–ã™ã‚‹ã‹ã€ãã®å“è³ªã‚’ç¢ºèªã—ã¾ã™ã€‚

- â‘  ç›®çš„ã¨å•é¡Œè¨­å®šã€é‡è¦ãªæ³¨æ„ç‚¹ã®ã¿ã‚’è¨˜è¿°

    :::details 01_simple_requirements.txt
    ```text
    # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ãƒ»æŽ¨è«–notebookã®è¦ä»¶
    - ä½¿ç”¨ãƒ‡ãƒ¼ã‚¿
        - ./data.csv
    - æ¬¡ã®ã‚ˆã†ãªå•é¡Œè¨­å®šã¨ã™ã‚‹
        - äºˆæ¸¬æ—¥ã‚’åŸºæº–ã«ã€å‰æ—¥ã¾ã§ã®30æ—¥é–“ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã£ã¦å‘ã“ã†30æ—¥é–“ã®è³¼å…¥é‡‘é¡ã‚’äºˆæ¸¬ã™ã‚‹
        - CustomerIDã”ã¨ã«é›†è¨ˆã‚’è¡Œã„ã€ç‰¹å¾´é‡ã¨ç›®çš„å¤‰æ•°ã‚’ä½œæˆã™ã‚‹
    - äº‹å‰ã«ãƒ‡ãƒ¼ã‚¿ç¢ºèªã‚’è¡Œã„ã€å¿…è¦ã¨è€ƒãˆã‚‰ã‚Œã‚‹å‰å‡¦ç†ã‚’è¡Œã†
    - æ¤œè¨¼è¨­è¨ˆ
        - å­¦ç¿’ãƒ»æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®åˆ†å‰²æ–¹æ³•ã¯ã€ãƒ‡ãƒ¼ã‚¿ç‰¹æ€§ã«åˆã‚ã›ã¦æ±ºã‚ã¦
        - ãƒªãƒ¼ã‚¯ã¯çµ¶å¯¾ã«é¿ã‘ã‚‹ã‚ˆã†å¾¹åº•çš„ã«ç¢ºèªã—ã¦
    - ãã®ä»–
        - ã‚³ãƒ¼ãƒ‰ã¯ `01_simple.py` ã«è¨˜è¿°
        - å‹•ä½œç¢ºèªã¯ `uv run 01_simple.py` ã§å®Ÿè¡Œ
    ```
    :::

- â‘¡ â‘ ã«åŠ ãˆã¦ã€è©¦è¡ŒéŒ¯èª¤ã•ã›ã¦æ”¹å–„ã‚’ä¿ƒã™

    :::details 02_try_and_error_requirements.txt
    ```text
    # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ãƒ»æŽ¨è«–notebookã®è¦ä»¶
    - ä½¿ç”¨ãƒ‡ãƒ¼ã‚¿
        - ./data.csv
    - æ¬¡ã®ã‚ˆã†ãªå•é¡Œè¨­å®šã¨ã™ã‚‹
        - äºˆæ¸¬æ—¥ã‚’åŸºæº–ã«ã€å‰æ—¥ã¾ã§ã®30æ—¥é–“ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã£ã¦å‘ã“ã†30æ—¥é–“ã®è³¼å…¥é‡‘é¡ã‚’äºˆæ¸¬ã™ã‚‹
        - CustomerIDã”ã¨ã«é›†è¨ˆã‚’è¡Œã„ã€ç‰¹å¾´é‡ã¨ç›®çš„å¤‰æ•°ã‚’ä½œæˆã™ã‚‹
    - äº‹å‰ã«ãƒ‡ãƒ¼ã‚¿ç¢ºèªã‚’è¡Œã„ã€å¿…è¦ã¨è€ƒãˆã‚‰ã‚Œã‚‹å‰å‡¦ç†ã‚’è¡Œã†
    - æ¤œè¨¼è¨­è¨ˆ
        - å­¦ç¿’ãƒ»æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®åˆ†å‰²æ–¹æ³•ã¯ã€ãƒ‡ãƒ¼ã‚¿ç‰¹æ€§ã«åˆã‚ã›ã¦æ±ºã‚ã¦
        - ãƒªãƒ¼ã‚¯ã¯çµ¶å¯¾ã«é¿ã‘ã‚‹ã‚ˆã†å¾¹åº•çš„ã«ç¢ºèªã—ã¦
    - ãã®ä»–
        - è¨­è¨ˆã€å®Ÿè£…ã—ãŸã¨ã“ã‚ã§ã€ãã®å†…å®¹ãŒé©åˆ‡ã‹ã©ã†ã‹ã‚’ã‚ˆãè€ƒãˆã€å‹•ä½œç¢ºèªã—ã¦ã€å¿…è¦ã«å¿œã˜ã¦æ”¹å–„ãƒ»ä¿®æ­£ã™ã‚‹
        - ã‚³ãƒ¼ãƒ‰ã¯ `02_try_and_error.py` ã«è¨˜è¿°
        - å‹•ä½œç¢ºèªã¯ `uv run 02_try_and_error.py` ã§å®Ÿè¡Œ

    ```
    :::

- â‘¢ â‘ ã«åŠ ãˆã¦ã€ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚„ç‰¹å¾´é‡ã«ã¤ã„ã¦è©³ã—ãè¨˜è¿°ï¼ˆ=ç­†è€…ãŒã‚„ã‚ŠãŸã„ã“ã¨ã«ç›¸å½“ï¼‰

    :::details 03_detail_requirements.txt
    ```text
    # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ãƒ»æŽ¨è«–notebookã®è¦ä»¶
    - ä½¿ç”¨ãƒ‡ãƒ¼ã‚¿
        - ./data.csv
    - æ¬¡ã®ã‚ˆã†ãªå•é¡Œè¨­å®šã¨ã™ã‚‹
        - äºˆæ¸¬æ—¥ã‚’åŸºæº–ã«ã€ã€Œå‰æ—¥ã¾ã§ã®30æ—¥é–“ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã£ã¦å‘ã“ã†30æ—¥é–“ã®è³¼å…¥é‡‘é¡ã‚’äºˆæ¸¬ã™ã‚‹ã€ã¨ã„ã†è¨­å®šã¨ã™ã‚‹
            - ã“ã®è¨­å®šã¯ã€ãƒ‡ãƒ¼ã‚¿ã®æ€§è³ªã‚„æ¥­å‹™ãƒ—ãƒ­ã‚»ã‚¹ã€é‹ç”¨æ–¹æ³•ãªã©ã«ã‚ˆã£ã¦å¤‰åŒ–ã™ã‚‹ã“ã¨ã‚‚ã‚ã‚‹ã®ã§ã€å®Ÿéš›ã®åˆ¶ç´„ã«ã‚‚ã¨ã¥ã„ã¦ã‚ˆã‚Šè‰¯ã„ã‚„ã‚Šæ–¹ã‚’è€ƒãˆã¦ã¿ã¦ãã ã•ã„
        - ä¾‹ãˆã°ã€2011-03-01 ã«äºˆæ¸¬æ—¥ã¨ã™ã‚‹å ´åˆ
            - InvoiceDateãŒ2011-02-28ä»¥å‰ã®90æ—¥é–“ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’é›†è¨ˆã—ã¦ç‰¹å¾´é‡ã‚’ä½œæˆã™ã‚‹
            - InvoiceDateãŒ2011-03-01ä»¥é™ã®30æ—¥é–“ã®è³¼å…¥é‡‘é¡ã®åˆè¨ˆã‚’ç›®çš„å¤‰æ•°ã¨ã™ã‚‹
        - äºˆæ¸¬æ—¥ã‚’ã‚·ãƒ•ãƒˆã™ã‚‹ã“ã¨ã§ã“ã®ã‚ˆã†ãªç‰¹å¾´é‡ã€ç›®çš„å¤‰æ•°ã®ã‚»ãƒƒãƒˆã‚’è¤‡æ•°ä½œã‚Œã‚‹ã®ã§ã€ãã‚Œã‚‰ã‚’çµ„ã¿åˆã‚ã›ã¦å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã€æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨æ„ã™ã‚‹
    - äº‹å‰ã«è¡Œã£ãŸãƒ‡ãƒ¼ã‚¿ç¢ºèªã®çµæžœã‚’ã‚‚ã¨ã«ã€æ¬¡ã®ã‚ˆã†ãªå‰å‡¦ç†ã‚’è¡Œã†
        - `pd.read_csv` ã§ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€éš›ã€`encoding='shift-jis'` ã‚’æŒ‡å®šã—ã¦ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼ã‚’å›žé¿
        - `CustomerID` ã§åž‹æŒ‡å®šï¼ˆ`str` ï¼‰
        - `InvoiceDate` ã‚«ãƒ©ãƒ ã‚’datetimeåž‹ã«å¤‰æ›
        - `CustomerID` ãŒæ¬ æã—ã¦ã„ã‚‹ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’å‰Šé™¤
        - `Quantity` ãŒãƒžã‚¤ãƒŠã‚¹å€¤ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’å‰Šé™¤
    - ã¾ãšã¯ç°¡å˜ãªç‰¹å¾´é‡ã§ã€ç²¾åº¦è©•ä¾¡ã‚’è¡Œãˆã‚‹ã‚ˆã†ã«ã™ã‚‹
        - ç‰¹å¾´é‡ã€ç›®çš„å¤‰æ•°ã¨ã‚‚ã€é›†è¨ˆã¯CustomerIDã¨prediction_dateã§ä¸€æ„ã¨ã™ã‚‹
        - ç‰¹å¾´é‡
            - ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°
            - InvoiceNoã®nunique
            - StockCodeã®nunique
            - InvoiceDateã®nunique
            - Countryã®mode
            - ã“ã‚Œã¾ã§ã®è³¼å…¥é‡‘é¡ã®sum, max, min, median
        - ç›®çš„å¤‰æ•°
            - (UnitPrice * Quantity) ã®åˆè¨ˆ
        - äº¤å·®æ¤œè¨¼
            
            | Fold | å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ | æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ |
            | --- | --- | --- |
            | 1 | â€2011-03-01â€, â€œ2011-04-01â€, â€œ2011-05-01â€ | â€œ2011-06-01â€ |
            | 2 | â€œ2011-04-01â€, â€œ2011-05-01â€, â€œ2011-06-01â€ | â€œ2011-07-01â€ |
            | 3 | â€œ2011-05-01â€, â€œ2011-06-01â€, â€œ2011-07-01â€ | â€œ2011-08-01â€ |
            | 4 | â€œ2011-06-01â€, â€œ2011-07-01â€, â€2011-08-01â€ | â€œ2011-09-01â€ |
            | 5 | â€œ2011-07-01â€, â€2011-08-01â€, â€œ2011-09-01â€ | â€œ2011-10-01â€ |
            | test | â€2011-08-01â€, â€œ2011-09-01â€, â€œ2011-10-01â€ | â€œ2011-11-01â€ |

        - ç›®çš„é–¢æ•°ã€è©•ä¾¡æŒ‡æ¨™ã¯å›žå¸°ã§ã‚ˆãä½¿ã‚ã‚Œã‚‹RMSEã‚’ç”¨ã„ã‚‹
        - ãƒ¢ãƒ‡ãƒ«ã¯LightGBMã‚’ä½¿ã†
            - scikit learnãƒ©ãƒƒãƒ‘ãƒ¼ã§ã¯ãªãã€LightGBM ã® Python APIã‚’ä½¿ç”¨
            - ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯Kaggleã®å…¬é–‹notebookã§ä½¿ç”¨ã•ã‚Œã‚‹è¨­å®šã‚’ä½¿ã†
        - ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
            - ã‚«ãƒ†ã‚´ãƒªå€¤ã®ç‰¹å¾´é‡ã¯ã€ãƒ¢ãƒ‡ãƒ«ã«å…¥åŠ›ã™ã‚‹å‰ã«LabelEncoderã§å¤‰æ›
            - å­¦ç¿’ã«ã‚ˆã£ã¦å¾—ã‚‰ã‚ŒãŸartifactsï¼ˆlgbãƒ¢ãƒ‡ãƒ«ã€label encoderã€ç‰¹å¾´é‡ã‚«ãƒ©ãƒ ï¼‰ã¯ä¿å­˜ã—ã¦ã€æŽ¨è«–æ™‚ã«ãƒ­ãƒ¼ãƒ‰ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹
            - æŽ¨è«–ã®ã¿ã§ã‚‚å®Ÿè¡Œã™ã‚‹ã‚ˆã†ãªä½œã‚Šã«ã—ã¦
    - ãã®ä»–
        - è¨­è¨ˆã€å®Ÿè£…ã—ãŸã¨ã“ã‚ã§ã€ãã®å†…å®¹ãŒé©åˆ‡ã‹ã©ã†ã‹ã‚’ã‚ˆãè€ƒãˆãŸã‚Šå‹•ä½œç¢ºèªã—ã¦ã€å¿…è¦ã«å¿œã˜ã¦è‡ªã‚‰æ”¹å–„ã—ã¦ãã ã•ã„
        - ã‚³ãƒ¼ãƒ‰ã¯ `03_detail.py` ã«è¨˜è¿°
        - å‹•ä½œç¢ºèªã¯ `uv run 03_detail.py` ã§å®Ÿè¡Œ
    ```
    :::

Gemini CLI ã§ã¯åˆ¥é€”ä½œæˆã—ãŸè¦ä»¶ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ‡å®šã—ãªãŒã‚‰ã‚³ãƒ¼ãƒ‰ã®å®Ÿè£…ã‚’æŒ‡ç¤ºã™ã‚‹ã‚ˆã†ã«ã—ã¾ã—ãŸã€‚åŸºæœ¬ã¨ãªã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯ä»¥ä¸‹ã®é€šã‚Šã§ã€`@01_simple_requirements.txt` ã®éƒ¨åˆ†ã‚’å„æ¡ä»¶ã«åˆã‚ã›ã¦å¤‰æ›´ã—ã¦ã„ã¾ã™ã€‚

```text
ã¾ãš @01_simple_requirements.txt ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚
ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚‚ã¨ã«ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã€æŽ¨è«–ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚
```

## â‘  ç›®çš„ã¨å•é¡Œè¨­å®šã€é‡è¦ãªæ³¨æ„ç‚¹ã®ã¿ã®å ´åˆ

ã¾ãšã¯ã€ä¸€ç™ºå‡ºã—ã§ã©ã®ã‚ˆã†ãªã‚³ãƒ¼ãƒ‰ãŒå‡ºã¦ãã‚‹ã‹ã‚’ç¢ºèªã—ã¾ã™ã€‚è¦ä»¶ã«ã¯ç›®çš„ã¨æœ€ä½Žé™ã®æ³¨æ„ç‚¹ã ã‘ã‚’è¨˜è¼‰ã—ã¾ã—ãŸã€‚
ç”Ÿæˆã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ã€‚

:::details â‘  ç›®çš„ã¨å•é¡Œè¨­å®šã®ã¿ã®å ´åˆã®ã‚³ãƒ¼ãƒ‰
```python
# /// script
# requires-python = ">=3.9"
dependencies = [
    "pandas",
    "scikit-learn",
    "numpy",
]
# ///

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score


import warnings

# è­¦å‘Šã‚’ç„¡è¦–ï¼ˆFutureWarningãªã©ï¼‰
warnings.filterwarnings('ignore')

def load_and_preprocess(filepath):
    """ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨åŸºæœ¬å‰å‡¦ç†"""
    print("Loading data...")
    # æ—¥ä»˜è§£æžã‚’å«ã‚ã¦èª­ã¿è¾¼ã¿
    try:
        df = pd.read_csv(filepath, parse_dates=['InvoiceDate'], encoding='ISO-8859-1')
    except UnicodeDecodeError:
        df = pd.read_csv(filepath, parse_dates=['InvoiceDate'], encoding='cp1252')
    
    print(f"Original data shape: {df.shape}")
    
    # CustomerIDãŒãªã„ãƒ‡ãƒ¼ã‚¿ã¯é¡§å®¢ã”ã¨ã®äºˆæ¸¬ã«ä½¿ãˆãªã„ãŸã‚å‰Šé™¤
    df = df.dropna(subset=['CustomerID'])
    df['CustomerID'] = df['CustomerID'].astype(int)
    
    # é‡‘é¡è¨ˆç®—
    df['TotalAmount'] = df['Quantity'] * df['UnitPrice']
    
    # æ—¥ä»˜ã®ã¿ã®ã‚«ãƒ©ãƒ ä½œæˆï¼ˆæ™‚é–“ã¯åˆ‡ã‚Šæ¨ã¦ï¼‰
    df['Date'] = df['InvoiceDate'].dt.normalize()
    
    print(f"Data shape after dropping null CustomerID: {df.shape}")
    print(f"Date range: {df['Date'].min()} to {df['Date'].max()}")
    
    return df

def create_features_and_target(df, cutoff_date, target_days=30, feature_days=30):
    """
    æŒ‡å®šã•ã‚ŒãŸåŸºæº–æ—¥(cutoff_date)ã«åŸºã¥ã„ã¦ç‰¹å¾´é‡ã¨ç›®çš„å¤‰æ•°ã‚’ä½œæˆã™ã‚‹
    
    ç‰¹å¾´é‡: [cutoff_date - feature_days, cutoff_date) ã®æœŸé–“ã®ãƒ‡ãƒ¼ã‚¿
    ç›®çš„å¤‰æ•°: [cutoff_date, cutoff_date + target_days) ã®æœŸé–“ã®ãƒ‡ãƒ¼ã‚¿
    """
    
    # æœŸé–“å®šç¾©
    feature_start = cutoff_date - pd.Timedelta(days=feature_days)
    feature_end = cutoff_date # åŸºæº–æ—¥ã®å‰æ—¥ã¾ã§ï¼ˆåŸºæº–æ—¥ã¯å«ã¾ãªã„ã€ã¾ãŸã¯åŸºæº–æ—¥ã®00:00ã¾ã§ï¼‰
    
    target_start = cutoff_date
    target_end = cutoff_date + pd.Timedelta(days=target_days)
    
    # ãƒ‡ãƒ¼ã‚¿ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    # ç‰¹å¾´é‡ç”¨ãƒ‡ãƒ¼ã‚¿
    feat_df = df[(df['Date'] >= feature_start) & (df['Date'] < feature_end)]
    
    # ç›®çš„å¤‰æ•°ç”¨ãƒ‡ãƒ¼ã‚¿
    target_df = df[(df['Date'] >= target_start) & (df['Date'] < target_end)]
    
    # ãƒ™ãƒ¼ã‚¹ã¨ãªã‚‹é¡§å®¢ãƒªã‚¹ãƒˆï¼ˆç‰¹å¾´é‡æœŸé–“ã«å­˜åœ¨ã—ãŸé¡§å®¢ã‚’ãƒ™ãƒ¼ã‚¹ã«ã™ã‚‹ã‹ã€å…¨é¡§å®¢ã«ã™ã‚‹ã‹ï¼‰
    # ã“ã“ã§ã¯ã€Œç‰¹å¾´é‡æœŸé–“ã«ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãŒã‚ã£ãŸé¡§å®¢ã€ã¾ãŸã¯ã€Œã‚¿ãƒ¼ã‚²ãƒƒãƒˆæœŸé–“ã«ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãŒã‚ã£ãŸé¡§å®¢ã€ã®ä¸¡æ–¹ã‚’è€ƒæ…®ã™ã¹ãã ãŒã€
    # ã€Œäºˆæ¸¬æ—¥ã‚’åŸºæº–ã«...è³¼å…¥é‡‘é¡ã‚’äºˆæ¸¬ã™ã‚‹ã€ãªã®ã§ã€
    # å®Ÿéš›ã«ã¯ã€Œäºˆæ¸¬æ™‚ç‚¹ã§ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªé¡§å®¢ã€ã‚ã‚‹ã„ã¯ã€Œå…¨é¡§å®¢ã€ãŒå¯¾è±¡ã€‚
    # ä»Šå›žã¯ã‚·ãƒ³ãƒ—ãƒ«ã«ã€ç‰¹å¾´é‡æœŸé–“ã¾ãŸã¯ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæœŸé–“ã«ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã™ã‚‹é¡§å®¢ã‚’å¯¾è±¡ã¨ã™ã‚‹ã€‚
    # ãŸã ã—ã€å­¦ç¿’æ™‚ã¯ã€Œç‰¹å¾´é‡ãŒå¾—ã‚‰ã‚Œã‚‹é¡§å®¢ã€ã«å¯¾ã—ã¦ã€Œæœªæ¥ã®å£²ä¸Šã€ã‚’ç´ä»˜ã‘ã‚‹å½¢å¼ãŒä¸€èˆ¬çš„ã€‚
    # ã‚‚ã—ç‰¹å¾´é‡æœŸé–“ã«è³¼å…¥ãŒãªãã¦ã‚‚ã€éŽåŽ»ã«ç™»éŒ²ãŒã‚ã‚Œã°äºˆæ¸¬å¯¾è±¡ã‹ã‚‚ã—ã‚Œãªã„ãŒã€
    # ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã¯é¡§å®¢ãƒžã‚¹ã‚¿ãŒãªã„ãŸã‚ã€ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ãŒã‚ã‚‹é¡§å®¢ã—ã‹ã‚ã‹ã‚‰ãªã„ã€‚
    # ã‚ˆã£ã¦ã€feat_df ã«å­˜åœ¨ã™ã‚‹é¡§å®¢ã‚’ãƒ™ãƒ¼ã‚¹ã¨ã™ã‚‹ï¼ˆç›´è¿‘30æ—¥è³¼å…¥ãªã—=0å††ã¨ã¿ãªã™ãªã‚‰ã€ãƒ™ãƒ¼ã‚¹ã‚’åºƒã’ã‚‹å¿…è¦ãŒã‚ã‚‹ï¼‰ã€‚
    
    # ä»Šå›žã¯ã€Œç›´è¿‘30æ—¥é–“ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã£ã¦ã€ã¨ã„ã†è¦ä»¶ãªã®ã§ã€
    # ç‰¹å¾´é‡æœŸé–“ã«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹é¡§å®¢ã‚’ãƒ¡ã‚¤ãƒ³ã®æ¯é›†å›£ã¨ã—ã¤ã¤ã€
    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæœŸé–“ã®ã¿ã«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹ï¼ˆæ–°è¦ãƒ»å¾©å¸°ï¼‰ã¯ã€éŽåŽ»ãƒ‡ãƒ¼ã‚¿ãŒãªã„ãŸã‚äºˆæ¸¬å›°é›£ã¨ã—ã¦ä»Šå›žã¯å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰é™¤å¤–ã€
    # ã‚ã‚‹ã„ã¯ç‰¹å¾´é‡0ã¨ã—ã¦æ‰±ã†ã“ã¨ãŒè€ƒãˆã‚‰ã‚Œã‚‹ã€‚
    # ã‚·ãƒ³ãƒ—ãƒ«åŒ–ã®ãŸã‚ã€ã€Œç‰¹å¾´é‡æœŸé–“ã«è³¼å…¥å®Ÿç¸¾ãŒã‚ã‚‹é¡§å®¢ã€ã‚’å¯¾è±¡ã¨ã™ã‚‹ã€‚
    
    base_customers = feat_df['CustomerID'].unique()
    
    # ç‰¹å¾´é‡é›†è¨ˆ
    features = feat_df.groupby('CustomerID').agg({
        'TotalAmount': ['sum', 'mean', 'count'],
        'Quantity': ['sum'],
        'InvoiceDate': ['max'] # æœ€çµ‚è³¼å…¥æ—¥è¨ˆç®—ç”¨
    })
    features.columns = ['_'.join(col).strip() for col in features.columns.values]
    
    # Recencyï¼ˆåŸºæº–æ—¥ - æœ€çµ‚è³¼å…¥æ—¥ï¼‰
    features['Recency'] = (cutoff_date - features['InvoiceDate_max'].dt.normalize()).dt.days
    features = features.drop('InvoiceDate_max', axis=1)
    
    # ã‚«ãƒ©ãƒ åã®ãƒªãƒãƒ¼ãƒ 
    features = features.add_prefix('Past30_')
    
    # ç›®çš„å¤‰æ•°é›†è¨ˆ
    targets = target_df.groupby('CustomerID')['TotalAmount'].sum().rename('Target_Next30_Amount')
    
    # çµåˆ (Left Join: ç‰¹å¾´é‡ãŒã‚ã‚‹é¡§å®¢ã«å¯¾ã—ã¦ã€æ­£è§£ã‚’ç´ä»˜ã‘ã‚‹ã€‚æ­£è§£ãŒãªã„å ´åˆã¯0å††ã¨ã™ã‚‹)
    data = pd.DataFrame(index=base_customers)
    data.index.name = 'CustomerID'
    
    data = data.join(features, how='left')
    data = data.join(targets, how='left')
    
    # æ¬ æå€¤åŸ‹ã‚
    # ç‰¹å¾´é‡ã¯ base_customers ãŒ feat_df ã‹ã‚‰æ¥ã¦ã„ã‚‹ã®ã§æ¬ æã¯ãªã„ã¯ãšã ãŒå¿µã®ãŸã‚
    data = data.fillna(0)
    
    return data

def main():
    # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    df = load_and_preprocess('data.csv')
    
    # ãƒ‡ãƒ¼ã‚¿ã®æœ€å¤§æ—¥ä»˜ã‚’ç¢ºèª
    max_date = df['Date'].max()
    min_date = df['Date'].min()
    print(f"Data covers from {min_date} to {max_date}")
    
    # 2. æ¤œè¨¼è¨­è¨ˆ
    # æœªæ¥ã®æƒ…å ±ã‚’ãƒªãƒ¼ã‚¯ã•ã›ãªã„ãŸã‚ã€æ™‚ç³»åˆ—ã§åˆ†å‰²ã™ã‚‹ã€‚
    # ãƒ†ã‚¹ãƒˆç”¨åŸºæº–æ—¥: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æœ€å¾Œã‹ã‚‰30æ—¥å‰ï¼ˆã“ã‚Œã«ã‚ˆã‚Šæœ€å¾Œã®30æ—¥é–“ã‚’ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æ­£è§£ã¨ã—ã¦ä½¿ãˆã‚‹ï¼‰
    test_cutoff = max_date - pd.Timedelta(days=30)
    
    print(f"\n--- Preparing Test Set (Cutoff: {test_cutoff}) ---")
    test_data = create_features_and_target(df, test_cutoff)
    print(f"Test data shape: {test_data.shape}")
    
    # å­¦ç¿’ç”¨åŸºæº–æ—¥: ãƒ†ã‚¹ãƒˆæœŸé–“ã¨ã‹ã¶ã‚‰ãªã„ã‚ˆã†ã«è¨­å®šã™ã‚‹ã€‚
    # ãƒ†ã‚¹ãƒˆæœŸé–“ã®ç‰¹å¾´é‡æœŸé–“: [test_cutoff - 30, test_cutoff)
    # ãƒ†ã‚¹ãƒˆæœŸé–“ã®ç›®çš„å¤‰æ•°æœŸé–“: [test_cutoff, test_cutoff + 30)
    # 
    # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¯ã€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ã€Œç‰¹å¾´é‡æœŸé–“ã€ä»¥å‰ã®æƒ…å ±ã®ã¿ã‚’ä½¿ã†ã®ãŒå®‰å…¨ã ãŒã€
    # äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«è‡ªä½“ã®å­¦ç¿’ã«ã¯ã€ŒéŽåŽ»ã®ç‰¹å¾´é‡ã€ã¨ã€ŒéŽåŽ»ã®æ­£è§£ï¼ˆãã®æ™‚ç‚¹ã§ã®æœªæ¥ï¼‰ã€ãŒã‚ã‚Œã°ã‚ˆã„ã€‚
    # ãƒªãƒ¼ã‚¯ã‚’é˜²ãã«ã¯ã€å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®ã€Œç›®çš„å¤‰æ•°æœŸé–“ã€ãŒã€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ã€Œç›®çš„å¤‰æ•°æœŸé–“ã€ã¨é‡ãªã‚‰ãªã„ï¼ˆã‚ã‚‹ã„ã¯ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ç‰¹å¾´é‡è¨ˆç®—æ™‚ç‚¹ã‚ˆã‚Šæœªæ¥ã®æƒ…å ±ã‚’å«ã¾ãªã„ï¼‰ã‚ˆã†ã«ã™ã‚‹ç­‰ã®é…æ…®ãŒå¿…è¦ã€‚
    # æœ€ã‚‚å®‰å…¨ãªã®ã¯ã€å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®ç›®çš„å¤‰æ•°æœŸé–“ã®çµ‚äº†æ—¥ãŒã€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®åŸºæº–æ—¥ä»¥å‰ã§ã‚ã‚‹ã“ã¨ã€‚
    # ã¤ã¾ã‚Š Train_Target_End <= Test_Cutoff
    # Train_Cutoff + 30 <= Test_Cutoff
    # Train_Cutoff <= Test_Cutoff - 30
    
    # ãƒ‡ãƒ¼ã‚¿ã‚’å¢—ã‚„ã™ãŸã‚ã«ã€30æ—¥ã‚¹ãƒ©ã‚¤ãƒ‰ã§ã„ãã¤ã‹å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆã™ã‚‹
    train_cutoffs = []
    current_cutoff = test_cutoff - pd.Timedelta(days=60) # ä½™è£•ã‚’æŒã£ã¦60æ—¥å‰ã‹ã‚‰ã‚¹ã‚¿ãƒ¼ãƒˆ
    
    while current_cutoff > min_date + pd.Timedelta(days=30):
        train_cutoffs.append(current_cutoff)
        current_cutoff -= pd.Timedelta(days=30) # 30æ—¥ãšã¤é¡ã‚‹
    
    print(f"\n--- Preparing Train Set (Cutoffs: {[t.date() for t in train_cutoffs]}) ---")
    
    train_dfs = []
    for cutoff in train_cutoffs:
        train_dfs.append(create_features_and_target(df, cutoff))
    
    if not train_dfs:
        print("Error: Not enough data to create training set.")
        return

    train_data = pd.concat(train_dfs)
    print(f"Train data shape: {train_data.shape}")
    
    # 3. ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
    X_train = train_data.drop('Target_Next30_Amount', axis=1)
    y_train = train_data['Target_Next30_Amount']
    
    X_test = test_data.drop('Target_Next30_Amount', axis=1)
    y_test = test_data['Target_Next30_Amount']
    
    print("\n--- Training Model ---")
    model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)
    model.fit(X_train, y_train)
    
    # 4. è©•ä¾¡
    print("\n--- Evaluation ---")
    y_pred = model.predict(X_test)
    
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    
    print(f"RMSE: {rmse:.4f}")
    print(f"MAE: {mae:.4f}")
    print(f"R2 Score: {r2:.4f}")
    
    # ç‰¹å¾´é‡é‡è¦åº¦
    print("\n--- Feature Importances ---")
    importances = pd.DataFrame({
        'feature': X_train.columns,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)
    print(importances)
    
    # å…·ä½“çš„ãªäºˆæ¸¬ä¾‹
    print("\n--- Prediction Examples (Top 5) ---")
    results = pd.DataFrame({
        'Actual': y_test,
        'Predicted': y_pred
    })
    print(results.head())

if __name__ == "__main__":
    main()
```
:::

### çµæžœã®è©•ä¾¡

ã‚³ãƒ¼ãƒ‰ã®ä¸­èº«ã‚’ç¢ºèªã—ã¦ã¿ã‚‹ã¨ã€ã„ãã¤ã‹ãƒã‚¸ãƒ†ã‚£ãƒ–ãªç‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚ã¾ãšæœ€åˆã®ç”Ÿæˆã§ `pd.read_csv` å®Ÿè¡Œæ™‚ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã™ã‚‹ã“ã¨ã‚’ç¢ºèªã—ãŸä¸Šã§ã€é©åˆ‡ãªã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’æŒ‡å®šã™ã‚‹ã‚ˆã†ã«ä¿®æ­£ã—ã¦ã„ã¾ã—ãŸã€‚ã¾ãŸã€å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æœŸé–“ã‚’ãšã‚‰ã—ã¦åˆ†å‰²ã—ã¦ãŠã‚Šã€ãƒªãƒ¼ã‚¯å¯¾ç­–ã‚‚æ„è­˜ã•ã‚Œã¦ã„ã¾ã™ã€‚ã•ã‚‰ã«ã€ãƒžãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°åˆ†æžã§ä¸€èˆ¬çš„ãªRFMåˆ†æžï¼ˆRecency, Frequency, Monetaryï¼‰ã®è¦³ç‚¹ã‚’å–ã‚Šå…¥ã‚Œã€Recencyã¨Monetaryã‚’è€ƒæ…®ã—ãŸç‰¹å¾´é‡ãŒä½œæˆã•ã‚Œã¦ã„ã¾ã—ãŸã€‚
ä¸€æ–¹ã§ã€æ”¹å–„ã™ã¹ãç‚¹ã‚‚ã„ãã¤ã‹è¦‹å—ã‘ã‚‰ã‚Œã¾ã™ã€‚ã¾ãšã€ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªããªã„ã«ã‚‚é–¢ã‚ã‚‰ãšã€æ±ŽåŒ–æ€§èƒ½ã‚’é«˜ã‚ã‚‹ãŸã‚ã«äº¤å·®æ¤œè¨¼ãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã§ã—ãŸã€‚ã¾ãŸã€ãƒ¢ãƒ‡ãƒ«ã«ã¯ Random Forest ãŒä½¿ç”¨ã•ã‚Œã¦ãŠã‚Šã€ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ‡ãƒ¼ã‚¿ã«ãŠã„ã¦ [Kaggle ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã§ã‚‚åˆæ‰‹ã¨ã—ã¦æŽ¨å¥¨](https://upura.hatenablog.com/entry/2019/10/29/184617) ã•ã‚Œã¦ã„ã‚‹ LightGBM ãªã©ã®å‹¾é…ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°ç³»ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ã¦ã»ã—ã„ã¨ã“ã‚ã§ã™ã€‚
ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ã§ã¯ã€äº‹å‰ã®ãƒ‡ãƒ¼ã‚¿ç¢ºèªã‚ˆã‚Š Quantity ã«ãƒžã‚¤ãƒŠã‚¹å€¤ãŒã‚ã‚‹ã“ã¨ãŒã‚ã‹ã£ã¦ãŠã‚Šã€ãã®ç†ç”±ã‚„èª¬æ˜ŽãŒè¦‹å½“ãŸã‚‰ãªã„ãŸã‚ãƒžã‚¤ãƒŠã‚¹å€¤ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã¯å‰Šé™¤ã—ã¦ãŠãã®ãŒç„¡é›£ã¨è€ƒãˆã‚‰ã‚Œã‚‹ä¸€æ–¹ã§ã€ç”Ÿæˆã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰ã§ã¯ãƒžã‚¤ãƒŠã‚¹å€¤ã®è€ƒæ…®ãŒãªãã€ãã®ã¾ã¾é›†è¨ˆã•ã‚Œã¦ã„ã¾ã™ã€‚Frequency ã«é–¢ã™ã‚‹ç‰¹å¾´é‡ã‚‚ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚æŽ¨è«–ã®ã¿è¡Œã†å‡¦ç†ãŒãªã‹ã£ãŸç‚¹ã‚‚ã‚‚ã†ä¸€æ­©ã¨ã„ã†ã¨ã“ã‚ã§ã™ã€‚

## â‘¡ è¨­è¨ˆæ¦‚è¦ã‚’ä¼ãˆã¦è©¦è¡ŒéŒ¯èª¤ã§æ”¹å–„ã‚’ä¿ƒã™

æ¬¡ã¯ã€â‘ ã®å†…å®¹ã«åŠ ãˆã¦ LLM è‡ªèº«ã§è©¦è¡ŒéŒ¯èª¤ã—ã¦æ”¹å–„ã™ã‚‹ã‚ˆã†ã«æŒ‡ç¤ºã‚’å‡ºã™ã‚ˆã†ãªæ¡ä»¶ã§ã™ã€‚LLMãŒè‡ªåˆ†ã§ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æžã—ã€ã‚ˆã‚Šè‰¯ã„ã‚³ãƒ¼ãƒ‰ã‚’æ›¸ã„ã¦ãã‚Œã‚‹ã“ã¨ã‚’æœŸå¾…ã—ã¾ã™ã€‚Gemini ã¯ã¾ãšã€ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æžã™ã‚‹ã‚³ãƒ¼ãƒ‰ã‚’ç”Ÿæˆãƒ»å®Ÿè¡Œã—ã¦ç¢ºèªã‚’è¡Œã„ã¾ã—ãŸã€‚

:::details â‘¡ è¨­è¨ˆæ¦‚è¦ã‚’ä¼ãˆã¦è©¦è¡ŒéŒ¯èª¤ã§æ”¹å–„ã‚’ä¿ƒã™å ´åˆã®ã‚³ãƒ¼ãƒ‰ï¼ˆãƒ‡ãƒ¼ã‚¿ç¢ºèªï¼‰
```python
# /// script
# requires-python = ">=3.10"
dependencies = [
    "pandas",
    "numpy",
]
# ///

import pandas as pd
import numpy as np

def analyze_data():
    print("Loading data...")
    # æ—¥ä»˜ãƒ‘ãƒ¼ã‚¹ã‚’æŒ‡å®šã—ã¦èª­ã¿è¾¼ã‚€
    try:
        df = pd.read_csv('./data.csv', parse_dates=['InvoiceDate'])
    except Exception as e:
        print(f"Failed to read csv with date parsing: {e}")
        # ãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆãŒç‰¹æ®Šã‹ã‚‚ã—ã‚Œãªã„ã®ã§æ™®é€šã«èª­ã‚€
        df = pd.read_csv('./data.csv')
        df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])

    print("\nData Info:")
    print(df.info())

    print("\nMissing Values:")
    print(df.isnull().sum())

    print("\nData Range:")
    print(f"Start: {df['InvoiceDate'].min()}")
    print(f"End: {df['InvoiceDate'].max()}")

    # CustomerIDãŒãªã„ãƒ‡ãƒ¼ã‚¿ã¯é¡§å®¢ã”ã¨ã®äºˆæ¸¬ãŒã§ããªã„ã®ã§ç¢ºèª
    missing_customer = df['CustomerID'].isnull().sum()
    print(f"\nMissing CustomerID count: {missing_customer} ({missing_customer/len(df):.2%})")

    # çµ±è¨ˆé‡
    print("\nDescriptive Stats:")
    print(df.describe())

if __name__ == "__main__":
    analyze_data()
```
:::

ã“ã®åˆ†æžçµæžœã‚’è¸ã¾ãˆã¦ç”Ÿæˆã•ã‚ŒãŸå­¦ç¿’ã‚³ãƒ¼ãƒ‰ãŒã“ã¡ã‚‰ã§ã™ã€‚

:::details â‘¡ è¨­è¨ˆæ¦‚è¦ã‚’ä¼ãˆã¦è©¦è¡ŒéŒ¯èª¤ã§æ”¹å–„ã‚’ä¿ƒã™å ´åˆã®ã‚³ãƒ¼ãƒ‰ï¼ˆå­¦ç¿’ãƒ»æŽ¨è«–ï¼‰
```python
# /// script
# requires-python = ">=3.10"
dependencies = [
    "pandas",
    "numpy",
    "scikit-learn",
    "lightgbm",
]
# ///

import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.metrics import mean_squared_error, mean_absolute_error
import warnings

# Suppress warnings for cleaner output
warnings.filterwarnings('ignore')

def load_data(file_path='./data.csv'):
    """
    Load data from CSV file. Handles different encodings.
    """
    print(f"Loading data from {file_path}...")
    try:
        # Try default UTF-8 first
        df = pd.read_csv(file_path, parse_dates=['InvoiceDate'])
    except UnicodeDecodeError:
        # Fallback to ISO-8859-1 (common for UK data)
        print("UTF-8 decode failed, trying ISO-8859-1...")
        df = pd.read_csv(file_path, encoding='ISO-8859-1', parse_dates=['InvoiceDate'])
    except Exception as e:
        print(f"Error loading data: {e}")
        return None
    return df

def preprocess(df):
    """
    Basic preprocessing:
    - Remove missing CustomerID
    - Calculate TotalAmount
    - Ensure correct data types
    """
    print("Preprocessing data...")
    
    # Drop missing CustomerID as we need to aggregate by customer
    initial_rows = len(df)
    df = df.dropna(subset=['CustomerID'])
    print(f"Dropped {initial_rows - len(df)} rows with missing CustomerID.")
    
    df['CustomerID'] = df['CustomerID'].astype(int)
    
    # Calculate TotalAmount
    df['TotalAmount'] = df['Quantity'] * df['UnitPrice']
    
    # Ensure InvoiceDate is datetime
    df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])
    
    return df

def create_features(df_history, ref_date):
    """
    Calculate features from historical data for each customer.
    """
    # Aggregations
    aggs = df_history.groupby('CustomerID').agg({
        'TotalAmount': ['sum', 'mean', 'count', 'std', 'min', 'max'],
        'Quantity': ['sum', 'mean'],
        'InvoiceDate': ['max'] # Used for Recency
    })
    
    # Flatten MultiIndex columns
    aggs.columns = ['_'.join(col).strip() for col in aggs.columns.values]
    
    # Recency: Days since last purchase
    # Note: (ref_date - last_purchase)
    aggs['Recency'] = (ref_date - aggs['InvoiceDate_max']).dt.days
    aggs = aggs.drop(columns=['InvoiceDate_max'])
    
    # Fill NaN values that might result from std calculation with single record
    aggs = aggs.fillna(0)
    
    return aggs

def create_dataset(df, lookback_days=30, target_days=30, step_days=7):
    """
    Create a dataset using a sliding window approach.
    
    Parameters:
    - lookback_days: Number of days to look back for features (X)
    - target_days: Number of days to look forward for target (y)
    - step_days: Step size for moving the window
    """
    start_date = df['InvoiceDate'].min()
    end_date = df['InvoiceDate'].max()
    
    print(f"Data Date Range: {start_date} to {end_date}")
    
    # Generate reference dates
    # Logic:
    # Feature Window: [ref_date - lookback, ref_date - 1]
    # Target Window:  [ref_date, ref_date + target - 1]
    
    # First ref_date must allow for full lookback
    first_ref = start_date + pd.Timedelta(days=lookback_days)
    # Last ref_date must allow for full target window
    last_ref = end_date - pd.Timedelta(days=target_days)
    
    if first_ref > last_ref:
        print("Error: Data duration is too short for the specified lookback and target periods.")
        return None, None, None

    ref_dates = pd.date_range(start=first_ref, end=last_ref, freq=f'{step_days}D')
    
    print(f"Generating dataset for {len(ref_dates)} reference dates (Sliding Window)...")
    
    X_list = []
    y_list = []
    meta_list = [] 
    
    # Sort dataframe once for faster slicing
    df = df.sort_values('InvoiceDate')
    
    for ref_date in ref_dates:
        # Define time windows
        feat_start = ref_date - pd.Timedelta(days=lookback_days)
        feat_end = ref_date - pd.Timedelta(days=1)
        
        target_start = ref_date
        target_end = ref_date + pd.Timedelta(days=target_days) # Exclusive in slice logic below if we use <
        
        # Slicing
        # Note: slice is inclusive for start, inclusive for end if using label-based indexing with datetime index,
        # but here we use boolean masking.
        # Features: [start, end] inclusive
        mask_feat = (df['InvoiceDate'] >= feat_start) & (df['InvoiceDate'] <= feat_end)
        df_feat = df.loc[mask_feat]
        
        # Target: [start, end) - let's be precise: target_days from ref_date
        mask_target = (df['InvoiceDate'] >= target_start) & (df['InvoiceDate'] < target_start + pd.Timedelta(days=target_days))
        df_target = df.loc[mask_target]
        
        if df_feat.empty:
            continue
            
        # 1. Calculate Features
        # We focus on customers who were active in the lookback period.
        features = create_features(df_feat, ref_date)
        
        # 2. Calculate Target
        # Sum of TotalAmount per customer in target window
        targets = df_target.groupby('CustomerID')['TotalAmount'].sum().rename('TargetAmount')
        
        # 3. Merge
        # Left join: We predict for customers observed in the feature window.
        # Customers who don't buy in the target window get 0 target amount.
        dataset = features.join(targets, how='left').fillna({'TargetAmount': 0})
        
        # Store metadata
        dataset['RefDate'] = ref_date
        
        X_list.append(dataset.drop(columns=['TargetAmount', 'RefDate']))
        y_list.append(dataset['TargetAmount'])
        meta_list.append(dataset[['RefDate']].reset_index()) # Includes CustomerID

    if not X_list:
        return None, None, None

    X = pd.concat(X_list)
    y = pd.concat(y_list)
    meta = pd.concat(meta_list)
    
    return X, y, meta

def train_and_evaluate(X, y, meta):
    """
    Split data based on time, train model, and evaluate.
    """
    # Chronological Split
    # Use the last 20% of reference dates for testing to avoid leakage
    unique_dates = sorted(meta['RefDate'].unique())
    split_idx = int(len(unique_dates) * 0.8)
    split_date = unique_dates[split_idx]
    
    print(f"\nSplitting data at RefDate: {split_date}")
    
    # Create masks based on RefDate aligned with X/y rows
    train_mask = meta['RefDate'] < split_date
    test_mask = meta['RefDate'] >= split_date
    
    X_train = X[train_mask.values]
    y_train = y[train_mask.values]
    X_test = X[test_mask.values]
    y_test = y[test_mask.values]
    
    print(f"Train set: {X_train.shape[0]} samples")
    print(f"Test set:  {X_test.shape[0]} samples")
    
    # Model Training (LightGBM)
    print("\nTraining LightGBM model...")
    model = lgb.LGBMRegressor(
        n_estimators=500,
        learning_rate=0.05,
        num_leaves=31,
        random_state=42,
        n_jobs=-1
    )
    model.fit(X_train, y_train)
    
    # Prediction
    preds = model.predict(X_test)
    
    # Metrics
    rmse = np.sqrt(mean_squared_error(y_test, preds))
    mae = mean_absolute_error(y_test, preds)
    
    print("------------------------------")
    print(f"RMSE: {rmse:.4f}")
    print(f"MAE:  {mae:.4f}")
    print("------------------------------")
    
    # Feature Importance
    importance = pd.DataFrame({
        'feature': X_train.columns,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    print("\nTop 5 Features:")
    print(importance.head(5))
    
    return model, preds, y_test

def main():
    # 1. Load
    df = load_data()
    if df is None:
        return

    # 2. Preprocess
    df = preprocess(df)
    
    # 3. Create Dataset
    print("\n--- Creating Dataset ---")
    X, y, meta = create_dataset(df, lookback_days=30, target_days=30, step_days=7)
    
    if X is None:
        print("Failed to create dataset.")
        return
        
    print(f"Total samples created: {X.shape[0]}")
    
    # 4. Train & Evaluate
    print("\n--- Model Training & Evaluation ---")
    train_and_evaluate(X, y, meta)

if __name__ == "__main__":
    main()
```
:::

### çµæžœã®è©•ä¾¡

æœŸå¾…ã—ã¦ã„ãŸã»ã©è©¦è¡ŒéŒ¯èª¤ã—ã¦ãã‚Œãªã‹ã£ãŸã®ã§ã™ãŒã€æœ€åˆã®å‡ºåŠ›ã§ä»¥ä¸‹ã®ã‚ˆã†ãªæ”¹å–„ãŒè¡Œã‚ã‚Œã€æœ€çµ‚çš„ãªã‚³ãƒ¼ãƒ‰ã«ãªã£ã¦ã„ã¾ã—ãŸã€‚
- ã‚³ãƒ¡ãƒ³ãƒˆè¿½åŠ 
- LightGBM ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¿½åŠ 
- `main` é–¢æ•°å†…ã®å‡¦ç†ã‚‚é©åˆ‡ã«é–¢æ•°åŒ–ãƒ»ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°

ã—ã‹ã—ã€ãƒã‚¸ãƒ†ã‚£ãƒ–ãªç‚¹ã‚„è¦æ”¹å–„ãªç‚¹ã®å¤§ç­‹ã¯ â‘  ã¨ã‚ã¾ã‚Šå¤‰ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ã€Œè©¦è¡ŒéŒ¯èª¤ã—ã¦ã€ã¨æŒ‡ç¤ºã‚’å«ã‚ã¦ã‚‚ã€å…·ä½“çš„ãªå®Ÿè£…ã‚´ãƒ¼ãƒ«ã‚„é‡è¦–ã™ã¹ãç‰¹å¾´é‡ãªã©ã®è¦ä»¶ãŒå®šç¾©ã•ã‚Œã¦ã„ãªã„ã¨æ˜Žã‚‰ã‹ãªæ”¹å–„ã«ã¯ç¹‹ãŒã‚Šã«ãã„ã‚ˆã†ã§ã™ã€‚


## â‘¢ è©³ç´°ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¨ç‰¹å¾´é‡ã‚’è¨˜è¿°

æœ€å¾Œã¯ã€äº‹å‰ã«äººé–“ãŒãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèªã—ãŸä¸Šã§ã€ãã®å†…å®¹ã‚’ã‚‚ã¨ã«å…·ä½“çš„ã«æŒ‡ç¤ºã‚’è¨˜è¿°ã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã™ã€‚â‘¢ã®è¦ä»¶ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«ã¯ã€ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æ§‹æˆã‚„ä½œæˆã™ã¹ãç‰¹å¾´é‡ã‚’ã€æ€ã„å½“ãŸã‚‹ã ã‘è¨˜è¿°ã—ã¾ã—ãŸã€‚

ç”Ÿæˆã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ã€‚

:::details â‘¢ è©³ç´°ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¨ç‰¹å¾´é‡ã‚’è¨˜è¿°
```python
# /// script
# requires-python = ">=3.9"
dependencies = [
    "pandas",
    "numpy",
    "lightgbm",
    "scikit-learn",
]
# ///

import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import LabelEncoder
import pickle
import os
import warnings

warnings.filterwarnings('ignore')

# è¨­å®š
INPUT_FILE = './data.csv'
ARTIFACTS_DIR = './artifacts'
os.makedirs(ARTIFACTS_DIR, exist_ok=True)

# ç‰¹å¾´é‡ç”Ÿæˆã®è¨­å®š
# è¦ä»¶æ›¸ã«ã¯ã€Œå‰æ—¥ã¾ã§ã®30æ—¥é–“ã®ãƒ‡ãƒ¼ã‚¿ã€ã¨ã‚ã‚‹ãŒã€ä¾‹ç¤ºã§ã¯ã€Œ90æ—¥é–“ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’é›†è¨ˆã€ã¨ã‚ã‚‹ãŸã‚ã€
# æƒ…å ±é‡ã‚’è€ƒæ…®ã—ä¾‹ç¤ºã®90æ—¥ã‚’æŽ¡ç”¨ã™ã‚‹ã€‚
FEATURE_WINDOW_DAYS = 90
TARGET_WINDOW_DAYS = 30

def load_and_preprocess(file_path):
    print("Loading data...")
    # pd.read_csv ã§ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€éš›ã€encoding='shift-jis' ã‚’æŒ‡å®š
    # CustomerIDã¯èª­ã¿è¾¼ã¿æ™‚ã«stræŒ‡å®šã§ããªã„å ´åˆãŒã‚ã‚‹(NAãŒã‚ã‚‹ã¨floatã«ãªã‚‹)ãŸã‚å¾Œã§å¤‰æ›
    df = pd.read_csv(file_path, encoding='shift-jis')
    
    # InvoiceDate ã‚«ãƒ©ãƒ ã‚’datetimeåž‹ã«å¤‰æ›
    df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])
    
    # CustomerID ãŒæ¬ æã—ã¦ã„ã‚‹ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’å‰Šé™¤
    df = df.dropna(subset=['CustomerID'])
    
    # CustomerIDã‚’strã«å¤‰æ› (ä¾‹ãˆã° 12345.0 -> "12345")
    df['CustomerID'] = df['CustomerID'].astype(float).astype(int).astype(str)
    
    # Quantity ãŒãƒžã‚¤ãƒŠã‚¹å€¤ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’å‰Šé™¤
    df = df[df['Quantity'] > 0]
    
    # é‡‘é¡åˆ—ã‚’è¿½åŠ 
    df['Amount'] = df['Quantity'] * df['UnitPrice']
    
    return df.reset_index(drop=True)

def create_features(df, prediction_date):
    """
    æŒ‡å®šã•ã‚ŒãŸ prediction_date ã‚’åŸºæº–ã«ç‰¹å¾´é‡ã‚’ä½œæˆã™ã‚‹
    å¯¾è±¡ãƒ‡ãƒ¼ã‚¿: [prediction_date - FEATURE_WINDOW_DAYS, prediction_date)
    """
    start_date = prediction_date - pd.Timedelta(days=FEATURE_WINDOW_DAYS)
    end_date = prediction_date
    
    mask = (df['InvoiceDate'] >= start_date) & (df['InvoiceDate'] < end_date)
    target_df = df[mask]
    
    if target_df.empty:
        return pd.DataFrame()

    # Countryã®modeã‚’å–å¾—ã™ã‚‹é–¢æ•°
    def get_mode(x):
        m = x.mode()
        if not m.empty:
            return m.iloc[0]
        return np.nan

    # é›†è¨ˆå®šç¾©
    # InvoiceNo: nunique
    # StockCode: nunique
    # InvoiceDate: nunique
    # Country: mode
    # Amount: sum, max, min, median
    # count: ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ã¨ã—ã¦ InvoiceNo ã® count ã‚’åˆ©ç”¨
    agg_funcs = {
        'InvoiceNo': ['count', 'nunique'], 
        'StockCode': ['nunique'],
        'InvoiceDate': ['nunique'],
        'Country': [get_mode],
        'Amount': ['sum', 'max', 'min', 'median']
    }
    
    grouped = target_df.groupby('CustomerID').agg(agg_funcs)
    
    # ã‚«ãƒ©ãƒ åã®å¹³å¦åŒ–
    new_cols = []
    for col, stat in grouped.columns:
        if stat == '<lambda>' or stat == 'get_mode': # get_mode
            stat = 'mode'
        if col == 'InvoiceNo' and stat == 'count':
            new_cols.append('record_count')
        else:
            new_cols.append(f'{col}_{stat}')
    
    grouped.columns = new_cols
    grouped = grouped.reset_index()
    
    # prediction_date ã‚’ä»˜ä¸Ž
    grouped['prediction_date'] = prediction_date
    
    return grouped

def create_target(df, prediction_date):
    """
    æŒ‡å®šã•ã‚ŒãŸ prediction_date ã‚’åŸºæº–ã«ç›®çš„å¤‰æ•°ã‚’ä½œæˆã™ã‚‹
    å¯¾è±¡ãƒ‡ãƒ¼ã‚¿: [prediction_date, prediction_date + TARGET_WINDOW_DAYS)
    """
    start_date = prediction_date
    end_date = prediction_date + pd.Timedelta(days=TARGET_WINDOW_DAYS)
    
    mask = (df['InvoiceDate'] >= start_date) & (df['InvoiceDate'] < end_date)
    target_df = df[mask]
    
    if target_df.empty:
        return pd.DataFrame(columns=['CustomerID', 'target_amount'])

    grouped = target_df.groupby('CustomerID')['Amount'].sum().reset_index()
    grouped.rename(columns={'Amount': 'target_amount'}, inplace=True)
    
    return grouped

def make_dataset(df, prediction_dates, is_train=True):
    features_list = []
    
    for date_str in prediction_dates:
        pred_date = pd.to_datetime(date_str)
        # print(f"Processing prediction date: {pred_date}")
        
        # ç‰¹å¾´é‡ä½œæˆ
        feats = create_features(df, pred_date)
        if feats.empty:
            continue
            
        if is_train:
            # ç›®çš„å¤‰æ•°ä½œæˆ
            targets = create_target(df, pred_date)
            # Left join: ç‰¹å¾´é‡ãŒã‚ã‚‹ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«å¯¾ã—ã¦ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚’çµåˆ
            merged = pd.merge(feats, targets, on='CustomerID', how='left')
            # æœŸé–“å†…ã«è³¼è²·ãŒãªã„ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯0å††ã¨ã™ã‚‹
            merged['target_amount'] = merged['target_amount'].fillna(0)
            features_list.append(merged)
        else:
            features_list.append(feats)
            
    if not features_list:
        return pd.DataFrame()
        
    return pd.concat(features_list, ignore_index=True)

def main():
    # 1. ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰
    df = load_and_preprocess(INPUT_FILE)
    print(f"Data loaded. Shape: {df.shape}")
    
    # 2. Foldå®šç¾©
    folds = [
        {
            'train': ["2011-03-01", "2011-04-01", "2011-05-01"],
            'valid': ["2011-06-01"]
        },
        {
            'train': ["2011-04-01", "2011-05-01", "2011-06-01"],
            'valid': ["2011-07-01"]
        },
        {
            'train': ["2011-05-01", "2011-06-01", "2011-07-01"],
            'valid': ["2011-08-01"]
        },
        {
            'train': ["2011-06-01", "2011-07-01", "2011-08-01"],
            'valid': ["2011-09-01"]
        },
        {
            'train': ["2011-07-01", "2011-08-01", "2011-09-01"],
            'valid': ["2011-10-01"]
        }
    ]
    
    # Test set definition
    test_set_def = {
        'train': ["2011-08-01", "2011-09-01", "2011-10-01"],
        'test': ["2011-11-01"]
    }

    # ãƒ©ãƒ™ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®æº–å‚™
    print("Preparing label encoder...")
    le = LabelEncoder()
    # æ¬ æã¯ãªã„å‰æã ãŒã€å¿µã®ãŸã‚astype(str)
    le.fit(df['Country'].astype(str).unique())
    
    # ä¿å­˜
    with open(os.path.join(ARTIFACTS_DIR, 'label_encoder.pkl'), 'wb') as f:
        pickle.dump(le, f)
    
    # ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    params = {
        'objective': 'regression',
        'metric': 'rmse',
        'boosting_type': 'gbdt',
        'learning_rate': 0.05,
        'num_leaves': 31,
        'feature_fraction': 0.9,
        'bagging_fraction': 0.8,
        'bagging_freq': 5,
        'verbose': -1,
        'seed': 42
    }

    # äº¤å·®æ¤œè¨¼ãƒ«ãƒ¼ãƒ—
    models = []
    rmse_scores = []
    feature_columns = None

    print("Starting cross-validation...")
    for i, fold in enumerate(folds):
        print(f"\n=== Fold {i+1} ===")
        
        # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ä½œæˆ
        train_df = make_dataset(df, fold['train'], is_train=True)
        valid_df = make_dataset(df, fold['valid'], is_train=True)
        
        if train_df.empty or valid_df.empty:
            print("Skipping fold due to empty data.")
            continue
            
        # ç‰¹å¾´é‡ã‚«ãƒ©ãƒ ã®ç‰¹å®š
        drop_cols = ['CustomerID', 'prediction_date', 'target_amount']
        features = [c for c in train_df.columns if c not in drop_cols]
        feature_columns = features # ä¿å­˜ç”¨ã«ä¿æŒ
        
        X_train = train_df[features].copy()
        y_train = train_df['target_amount']
        X_valid = valid_df[features].copy()
        y_valid = valid_df['target_amount']
        
        # ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã®å¤‰æ›
        if 'Country_mode' in X_train.columns:
            X_train['Country_mode'] = le.transform(X_train['Country_mode'].astype(str))
            X_valid['Country_mode'] = le.transform(X_valid['Country_mode'].astype(str))
            categorical_feats = ['Country_mode']
        else:
            categorical_feats = []

        # LGBM Dataset
        lgb_train = lgb.Dataset(X_train, y_train, categorical_feature=categorical_feats)
        lgb_eval = lgb.Dataset(X_valid, y_valid, reference=lgb_train, categorical_feature=categorical_feats)
        
        # å­¦ç¿’
        model = lgb.train(
            params,
            lgb_train,
            num_boost_round=1000,
            valid_sets=[lgb_train, lgb_eval],
            callbacks=[
                lgb.early_stopping(stopping_rounds=50),
                lgb.log_evaluation(period=50)
            ]
        )
        
        models.append(model)
        
        # è©•ä¾¡
        y_pred = model.predict(X_valid, num_iteration=model.best_iteration)
        rmse = np.sqrt(mean_squared_error(y_valid, y_pred))
        rmse_scores.append(rmse)
        print(f"Fold {i+1} RMSE: {rmse:.4f}")

    if rmse_scores:
        print(f"\nAverage RMSE: {np.mean(rmse_scores):.4f}")
    
    # Artifactsã®ä¿å­˜
    if models:
        print("\nSaving artifacts...")
        best_model = models[-1] # ç°¡æ˜“çš„ã«æœ€å¾Œã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
        best_model.save_model(os.path.join(ARTIFACTS_DIR, 'lgb_model.txt'))
        
        with open(os.path.join(ARTIFACTS_DIR, 'feature_columns.pkl'), 'wb') as f:
            pickle.dump(feature_columns, f)
        print(f"Artifacts saved to {ARTIFACTS_DIR}")

    # æŽ¨è«–ç”¨ãƒ†ã‚¹ãƒˆ
    print("\n=== Inference on Test Set (Simulation) ===")
    # Test setã®æ—¥ä»˜ã§äºˆæ¸¬ã‚’è¡Œã†
    test_date_str = test_set_def['test'][0]
    print(f"Predicting for date: {test_date_str}")
    
    test_features_df = make_dataset(df, [test_date_str], is_train=False)
    
    if not test_features_df.empty and os.path.exists(os.path.join(ARTIFACTS_DIR, 'feature_columns.pkl')):
        # ãƒ­ãƒ¼ãƒ‰
        with open(os.path.join(ARTIFACTS_DIR, 'feature_columns.pkl'), 'rb') as f:
            loaded_features = pickle.load(f)
        with open(os.path.join(ARTIFACTS_DIR, 'label_encoder.pkl'), 'rb') as f:
            loaded_le = pickle.load(f)
        loaded_model = lgb.Booster(model_file=os.path.join(ARTIFACTS_DIR, 'lgb_model.txt'))
        
        X_test = test_features_df[loaded_features].copy()
        
        if 'Country_mode' in X_test.columns:
            X_test['Country_mode'] = loaded_le.transform(X_test['Country_mode'].astype(str))
            
        preds = loaded_model.predict(X_test)
        test_features_df['predicted_amount'] = preds
        
        # è¡¨ç¤º
        print("Top 5 predictions:")
        print(test_features_df[['CustomerID', 'prediction_date', 'predicted_amount']].head())
        
        # è©•ä¾¡ (å®Ÿéš›ã®å€¤ã¨æ¯”è¼ƒ)
        print("Evaluating test set performance...")
        actual_target = create_target(df, pd.to_datetime(test_date_str))
        merged_res = pd.merge(test_features_df, actual_target, on='CustomerID', how='left')
        merged_res['target_amount'] = merged_res['target_amount'].fillna(0)
        
        test_rmse = np.sqrt(mean_squared_error(merged_res['target_amount'], merged_res['predicted_amount']))
        print(f"Test Set RMSE: {test_rmse:.4f}")

if __name__ == "__main__":
    main()
```
:::

### çµæžœã®è©•ä¾¡

è¦ä»¶ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«è¨˜è¿°ã—ãŸå†…å®¹ã¯æ¦‚ã­å®Ÿè£…ã«åæ˜ ã•ã‚Œã¦ã„ã‚‹ã‚‚ã®ã®ã€ç´°ã‹ãè¦‹ã‚‹ã¨æ”¹å–„ç‚¹ã¯æ®‹ã£ã¦ã„ã¾ã™ã€‚ä¾‹ãˆã°ã€LabelEncoder ã® `fit` ã‚’å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®åˆ†å‰²å‰ã«è¡Œã£ã¦ã„ã‚‹ãŸã‚ã€ãƒªãƒ¼ã‚¯ã®ãŠãã‚ŒãŒã‚ã‚Šã¾ã™ã€‚ã¾ãŸã€å­¦ç¿’ã®é€²æ—ãƒ­ã‚°ãŒå‡ºåŠ›ã•ã‚Œãªã„è¨­å®šã«ãªã£ã¦ãŠã‚Šã€å€‹äººçš„ã«ã¯å­¦ç¿’ä¸­ã®é€²æ—ã‚’ç¢ºèªã§ãã‚‹ã¨å¬‰ã—ã„ãªã¨æ€ã„ã¾ã™ï¼ˆã“ã®ç‚¹ã¯å¥½ã¿ãŒåˆ†ã‹ã‚Œã‚‹éƒ¨åˆ†ã‹ã¨æ€ã„ã¾ã™ï¼‰ã€‚
ãŸã ã€ã“ã®ã‚³ãƒ¼ãƒ‰ã§ã‚ã‚Œã°æ‰‹ç›´ã—ç¨‹åº¦ã§ç²¾åº¦æ”¹å–„ã®è©¦è¡ŒéŒ¯èª¤ãªã©ã®æ¬¡ã‚¹ãƒ†ãƒƒãƒ—ã®ä½œæ¥­ã«ç§»ã‚Œãã†ã§ã™ã€‚â‘ â‘¡ã®å ´åˆã«ã¯è¿½åŠ ã§å¿…è¦ãªæ‰‹ç›´ã—ãŒå¤šãã€çµæžœã€ä¼šè©±ã‚’ç¶™ç¶šã—ãŸã‚Šè‡ªèº«ã§è¿½åŠ å®Ÿè£…ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã€å€‹äººçš„ã«ã¯ï¼ˆåº¦ã€…ã®ä¼šè©±ã®å¾€å¾©ãŒãã“ã¾ã§å¥½ãã§ãªã„èƒŒæ™¯ã‚‚ã‚ã‚Šï¼‰â‘¢ã®æ–¹æ³•ãŒæœ€ã‚‚è¶£å‘³ã«åˆã„ãã†ã§ã—ãŸã€‚ã“ã®ã‚ãŸã‚Šã®åŒ™åŠ æ¸›ã¯å€‹ã€…äººã®è¶£å‘³å—œå¥½ã«å·¦å³ã•ã‚Œã‚‹ã¨ã‚‚æ€ã„ã¾ã™ã€‚


## ãŠã‚ã‚Šã«

ä»Šå›žã®å®Ÿé¨“ã§ã¯ã€æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ»æŽ¨è«–ã‚³ãƒ¼ãƒ‰ã®ç”ŸæˆAIã«ã‚ˆã‚‹ Vibe Modeling (å‹æ‰‹ã«å‘¼ç§°) ã¯ã€æœ€ä½Žé™ã®æŒ‡ç¤ºã§ã‚‚å¤§ããå¤–ã—ãŸã‚³ãƒ¼ãƒ‰ã«ã¯ãªã‚‰ãªã„ã‚‚ã®ã®ã€ã€Œã‚‚ã£ã¨ã“ã†ã—ã¦ã»ã—ã„ã€ã¨ã„ã†éƒ¨åˆ†ãŒæ®‹ã‚‹çµæžœã«ãªã‚Šã¾ã—ãŸã€‚ã‚„ã‚ŠãŸã„ã“ã¨ã‚’è©³ç´°ã«è¨˜è¿°ã™ã‚‹ã¨æ¦‚ã­å®Ÿè£…ã«åæ˜ ã•ã‚Œã‚‹ã®ã§ã™ãŒã€ã‚„ã‚ŠãŸã„ã“ã¨ã‚’è¨˜è¿°ã§ãã‚‹ã¾ã§ã«äººé–“å´ã§ãƒ‡ãƒ¼ã‚¿ç¢ºèªã‚’åœ°é“ã«è¡Œã†å¿…è¦ãŒã‚ã‚Šã€å½“ãŸã‚Šå‰ã§ã™ãŒäº‹å‰æº–å‚™ã®æ‰‹é–“ã¨ç”Ÿæˆç‰©ã®å“è³ªã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’æ„Ÿã˜ã¾ã—ãŸã€‚
å®Ÿè£…ãã®ã‚‚ã®ã®æ‰‹é–“ãŒçœã‘ã‚‹ã“ã¨ã‚„ã€ãƒ‡ãƒ¼ã‚¿ç¢ºèªã§ã‚‚ã‚³ãƒ¼ãƒ‰ç”Ÿæˆã‚’æ´»ç”¨ã§ãã‚‹ã®ã§ã€ã‚¼ãƒ­ã‹ã‚‰ã®å®Ÿè£…ã‚ˆã‚Šã¯ã‹ãªã‚Šæ¥½ã«ãªã‚‹ã‚‚ã®ã®ã€AIã¨äººé–“ã®ä½œæ¥­åˆ†æ‹…ã®ãƒãƒ©ãƒ³ã‚¹ã¯ã¾ã è¿·ã†ã¨ã“ã‚ãŒã‚ã‚‹ãªã¨ã„ã†ã®ãŒç¾åœ¨ã®æ­£ç›´ãªæ„Ÿæƒ³ã§ã™ã€‚
ä»Šå¾Œã‚‚ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã¯æ”¹å–„ã•ã‚Œã‚‹ã§ã—ã‚‡ã†ã—ã€å¼•ãç¶šãå¿ƒåœ°ã‚ˆã„ä½œæ¥­ãƒ•ãƒ­ãƒ¼ã‚„åˆ†æ‹…æ–¹é‡ã‚’æŽ¢ã£ã¦ã„ã‘ã‚Œã°ã¨æ€ã„ã¾ã™ã€‚

æ˜Žæ—¥ã® [JP_Google Developer Experts Advent Calendar 2025](https://adventar.org/calendars/11658) ã¯å²©å°¾ã•ã‚“ã®[ã€Gemini Canvasã€‘å° 1 å¨˜ã®æ¼¢æ¤œå¯¾ç­–ã«ã€Œææ€–ã®æ¼¢å­—é¬¼ã”ã£ã“ã€ã‚’çˆ†é€Ÿé–‹ç™ºã—ãŸã‚‰åŠ¹æžœã¦ãã‚ã‚“ã ã£ãŸè©±](https://zenn.dev/mbk_digital/articles/14e29841551dbc)ã§ã™ï¼ãŠæ¥½ã—ã¿ã«ï¼