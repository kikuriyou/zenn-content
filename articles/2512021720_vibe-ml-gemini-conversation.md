---
title: "Gemini CLI ã¨ Gemini 3.0 Pro ã§ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ‡ãƒ¼ã‚¿ã® Vibe Modeling (ã®ã‚ˆã†ãªä½•ã‹) ã‚’è©¦ã™ - ä¼šè©±ç·¨"
emoji: "ğŸ€"
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢
topics: ["googlecloud", "vertexai", "gemini"]
published: true
published_at: 2025-12-19 08:00
---

[JP_Google Developer Experts Advent Calendar 2025](https://adventar.org/calendars/11658) ã® 19 æ—¥ç›®ã®è¨˜äº‹ã§ã™ã€‚

## ã¯ã˜ã‚ã«

[å‰ã®è¨˜äº‹](https://zenn.dev/kikuriyou/articles/2511271718_vibe-ml-gemini)ã§ã¯ã€ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ãƒ»æ¨è«–ã‚³ãƒ¼ãƒ‰ã‚’ä¸€ç™ºå‡ºã—ã—ãŸå ´åˆã®ç”Ÿæˆçµæœã®å“è³ªã«ã¤ã„ã¦ç¢ºèªã—ã¾ã—ãŸã€‚
ã“ã®è¨˜äº‹ã§ã¯ã€ç°¡å˜ãªè¦ä»¶ã‹ã‚‰ã‚³ãƒ¼ãƒ‰ã‚’ç”Ÿæˆã—å§‹ã‚ã€ãã®å¾Œã‚‚ä¼šè©±ã‚’ç¶šã‘ã‚‹ã“ã¨ã§æœ¬æ¥ã‚„ã‚ŠãŸã„å†…å®¹ã«è¿‘ã¥ã‘ã‚‹ã“ã¨ã‚’è©¦ã—ã¾ã™ã€‚ãã®éš›ã€ã‚³ãƒ¼ãƒ‰ãã®ã‚‚ã®ã®å¤‰æ›´ã¯è‡ªåˆ†ã§ã¯è¡Œã‚ãšã€AIã¸ã®æŒ‡ç¤ºã®ã¿ã§é€²ã‚ã‚‹ãƒ«ãƒ¼ãƒ«ã¨ã—ã¾ã™ã€‚

ä¼šè©±ã‚’é€šã—ã¦ã©ã“ã¾ã§ç­†è€…ã®æ„å›³é€šã‚Šã«æ”¹å–„ã‚’è¡Œãˆã‚‹ã‹ã€ã¾ãŸå‰è¨˜äº‹ã§è¦ä»¶ã‚’è©³ã—ãè¨˜è¿°ã—ãŸå ´åˆã¨ã®ç”Ÿæˆçµæœã‚„ä½“æ„Ÿã®é•ã„ãªã©ã‚’ç¢ºèªã—ã¾ã™ã€‚

## å®Ÿé¨“ã®è¨­å®š

ç”Ÿæˆé–‹å§‹æ™‚ç‚¹ã®è¨­å®šã¯åŸºæœ¬çš„ã«å‰è¨˜äº‹ã¨åŒã˜å†…å®¹ã¨ã—ã¾ã™ã€‚

- ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ”¯æ´: Gemini CLI
- LLM: Gemini 3.0 Pro
- ãƒ‡ãƒ¼ã‚¿: [Online Retail II UCI](https://www.kaggle.com/datasets/mashlyn/online-retail-ii-uci)

ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã€æ¨è«–ã® Gemini CLI ã§ã®æŒ‡ç¤ºã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ã€‚å‰è¨˜äº‹ã® 1ï¼ˆç›®çš„ã¨å•é¡Œè¨­å®šã®ã¿ï¼‰ã¨åŒã˜æŒ‡ç¤ºã‹ã‚‰é–‹å§‹ã—ã¦ã€ãã®å¾Œã‚‚ä¼šè©±ã‚’ç¶šã‘ã¦æ”¹å–„ã‚’è©¦ã¿ã¾ã™ã€‚

```docker
ã¾ãš @01_simple_requirements.txt ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚
ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãƒ™ãƒ¼ã‚¹ã«ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã€æ¨è«–ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚
```

ä»¥é™å…¨ä½“ã‚’é€šã—ã¦ã€AIã«ã‚ˆã‚‹ã‚³ãƒ¼ãƒ‰ç”Ÿæˆã«ã¯è‡ªå‹• Lint ã«ã‚ˆã‚‹å¤‰æ›´ãŒå«ã¾ã‚Œã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚

## 1. ã‚„ã‚ŠãŸã„ã“ã¨ãŒæ±ºã¾ã£ã¦ã„ã‚‹å ´åˆ

ã¾ãšã¯ã€æœ€çµ‚çš„ã«ã‚„ã‚ŠãŸã„æ§‹æˆï¼ˆLightGBMã€äº¤å·®æ¤œè¨¼ã€ç‰¹å®šã®ç‰¹å¾´é‡ãªã©ï¼‰ãŒæ±ºã¾ã£ã¦ã„ã‚‹ãŒã€æœ€åˆã¯ç°¡å˜ãªæŒ‡ç¤ºã‹ã‚‰å§‹ã‚ã¦ã€å¾ã€…ã«æŒ‡ç¤ºã‚’è¿½åŠ ã—ã¦ã„ããƒ‘ã‚¿ãƒ¼ãƒ³ã§ã™ã€‚

### åˆæœŸç”Ÿæˆ

ã¾ãšã¯ä¸€ç™ºå‡ºã—ã®ã‚³ãƒ¼ãƒ‰ã‚’ç¢ºèªã—ã¾ã™ã€‚

:::details åˆæœŸç”Ÿæˆã‚³ãƒ¼ãƒ‰
```python
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, r2_score

def preprocess_data(df):
    df = df.copy() # Operate on a copy to avoid SettingWithCopyWarning
    # Convert InvoiceDate to datetime
    df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])

    # Calculate Amount
    df['Amount'] = df['Quantity'] * df['UnitPrice']

    # Drop rows with missing CustomerID
    df = df.dropna(subset=['CustomerID'])
    df['CustomerID'] = df['CustomerID'].astype(int)

    # Filter out returns (negative Quantity) and invalid prices/quantities
    df = df[(df['Quantity'] > 0) & (df['UnitPrice'] > 0)]
    
    # Sort by CustomerID and InvoiceDate for time-series operations
    df = df.sort_values(by=['CustomerID', 'InvoiceDate'])
    
    return df

def create_features_and_target(df, prediction_date):
    # Data for feature window (past 30 days relative to prediction_date)
    feature_start_date = prediction_date - pd.Timedelta(days=30)
    feature_end_date = prediction_date - pd.Timedelta(days=1) # Up to day before prediction date

    # Data for target window (next 30 days from prediction_date)
    target_start_date = prediction_date
    target_end_date = prediction_date + pd.Timedelta(days=29) # Inclusive, so 30 days

    # Features: aggregate past 30 days purchases for each customer
    features_df = df[(df['InvoiceDate'] >= feature_start_date) & (df['InvoiceDate'] <= feature_end_date)]
    customer_features = features_df.groupby('CustomerID').agg(
        last_30_days_total_amount=('Amount', 'sum'),
        last_30_days_order_count=('InvoiceNo', 'nunique'),
        last_30_days_item_count=('Quantity', 'sum')
    ).reset_index()
    
    # Target: aggregate next 30 days purchases for each customer
    target_df = df[(df['InvoiceDate'] >= target_start_date) & (df['InvoiceDate'] <= target_end_date)]
    customer_target = target_df.groupby('CustomerID').agg(
        next_30_days_total_amount=('Amount', 'sum')
    ).reset_index()

    # Merge features and target
    # Use outer merge to keep all customers, then fill NaNs for those who didn't purchase in a window
    # It's important to merge on all unique CustomerIDs present in either feature or target window for this prediction_date
    all_customer_ids = pd.concat([customer_features['CustomerID'], customer_target['CustomerID']]).unique()
    
    # Create a base dataframe with all relevant CustomerIDs for this prediction_date
    base_customers_df = pd.DataFrame({'CustomerID': all_customer_ids})

    merged_df = pd.merge(base_customers_df, customer_features, on='CustomerID', how='left')
    merged_df = pd.merge(merged_df, customer_target, on='CustomerID', how='left')
    
    # Fill NaN targets with 0 (customers who didn't purchase in the next 30 days)
    merged_df['next_30_days_total_amount'] = merged_df['next_30_days_total_amount'].fillna(0)
    
    # Fill NaN features with 0 (customers who didn't purchase in the past 30 days)
    merged_df = merged_df.fillna(0)

    return merged_df

def main():
    print("Loading data...")
    df = pd.read_csv('data.csv', encoding='latin1')
    print("Data loaded.")

    print("Preprocessing data...")
    df = preprocess_data(df)
    print("Data preprocessed.")

    min_date = df['InvoiceDate'].min()
    max_date = df['InvoiceDate'].max()
    print(f"Date range: {min_date} to {max_date}")

    # Define a cutoff date for training and validation to avoid data leakage.
    # The validation set will predict for dates *after* this cutoff.
    # Let's use 80% of the time range for training, 20% for validation.
    total_days = (max_date - min_date).days
    
    # Ensure train_end_date_for_generating_samples allows enough time for both feature and target windows
    # min_date + 30 days (features) + 30 days (target) = min_date + 60 days is the earliest actual prediction for a sample
    # So, ensure the split point is after this.
    
    # Let's use the last 3 months of data for validation.
    # So, training prediction dates will be before `max_date - 3 months`
    # Validation prediction dates will be after `max_date - 3 months`
    
    validation_start_prediction_date = max_date - pd.Timedelta(days=90) # approx 3 months before max date

    # Generate samples for training and validation
    all_data_points = []
    
    # Earliest possible prediction date that allows for a full 30-day feature window
    # and a full 30-day target window.
    # min_date + 30 days (features) -> then next day is prediction date.
    # Prediction date itself needs 30 days for target. So, min_date + 30 days (features) + 30 days (target)
    current_prediction_date = min_date + pd.Timedelta(days=60) # Start generating samples from here

    # Ensure that current_prediction_date + 29 days (for target window) does not exceed max_date
    while current_prediction_date + pd.Timedelta(days=29) <= max_date:
        # print(f"Creating features for prediction date: {current_prediction_date}") # Too verbose
        features_target = create_features_and_target(df, current_prediction_date)
        
        # Add the prediction date to the features_target df for splitting
        features_target['prediction_date'] = current_prediction_date
        all_data_points.append(features_target)
        
        current_prediction_date += pd.Timedelta(days=7) # Move prediction window by 7 days

    full_dataset = pd.concat(all_data_points, ignore_index=True)
    
    # Split the data into training and validation sets based on prediction_date
    train_df = full_dataset[full_dataset['prediction_date'] < validation_start_prediction_date]
    val_df = full_dataset[full_dataset['prediction_date'] >= validation_start_prediction_date]

    # Drop prediction_date and CustomerID as they are not features for the model
    X_train = train_df.drop(columns=['CustomerID', 'next_30_days_total_amount', 'prediction_date'])
    y_train = train_df['next_30_days_total_amount']

    X_val = val_df.drop(columns=['CustomerID', 'next_30_days_total_amount', 'prediction_date'])
    y_val = val_df['next_30_days_total_amount']
    
    print(f"Total generated samples: {len(full_dataset)}")
    print(f"Training data points (unique prediction dates): {train_df['prediction_date'].nunique()}")
    print(f"Validation data points (unique prediction dates): {val_df['prediction_date'].nunique()}")

    if len(X_train) == 0 or len(X_val) == 0:
        print("Not enough data to create proper training/validation sets with the chosen split. Consider adjusting validation_start_prediction_date or the overall date range.")
        print(f"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}")
        print(f"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}")
        return

    print("Training model...")
    # Using n_jobs=-1 to utilize all available CPU cores for faster training
    model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1) 
    model.fit(X_train, y_train)
    print("Model trained.")

    print("Evaluating model...")
    y_pred = model.predict(X_val)
    
    mae = mean_absolute_error(y_val, y_pred)
    r2 = r2_score(y_val, y_pred)
    
    print(f"Mean Absolute Error on validation set: {mae:.2f}")
    print(f"R^2 Score on validation set: {r2:.2f}")

    # --- Inference Example ---
    # To demonstrate inference, let's pick a date just after our validation period
    # This simulates making a prediction for a future period.
    inference_prediction_date = max_date + pd.Timedelta(days=1) # The day after the last available data
    
    # Ensure this inference date allows for a full 30-day feature window
    # If max_date is Dec 9, 2011, and inference_prediction_date is Dec 10, 2011.
    # The feature window would be Nov 10, 2011 - Dec 9, 2011. This uses all available data.
    
    # For target window, we assume we don't have this data yet, as it's future.
    # create_features_and_target can still be used, but the target part will be empty (all 0s), which is fine for feature extraction.
    
    print(f"\n--- Performing Inference for a future prediction date: {inference_prediction_date} ---")
    
    # Re-create features for this specific prediction date using the full original dataframe
    # The `create_features_and_target` function will correctly extract features based on `inference_prediction_date - 30 days` to `inference_prediction_date - 1 day`.
    inference_features_df = create_features_and_target(df, inference_prediction_date)
    
    # We need to ensure the columns match X_train, so drop CustomerID and the target
    # The target column will be all 0s for this inference_prediction_date, which is correct
    X_inference = inference_features_df.drop(columns=['CustomerID', 'next_30_days_total_amount'])
    
    # Predict
    if not X_inference.empty:
        inference_predictions = model.predict(X_inference)
        
        # Map predictions back to CustomerIDs
        inference_results = pd.DataFrame({
            'CustomerID': inference_features_df['CustomerID'],
            'Predicted_Next_30_Days_Amount': inference_predictions
        })
        
        print("\nTop 5 customers with highest predicted purchase amount for the next 30 days:")
        print(inference_results.sort_values(by='Predicted_Next_30_Days_Amount', ascending=False).head())

        print("\nBottom 5 customers with lowest (or zero) predicted purchase amount for the next 30 days:")
        print(inference_results.sort_values(by='Predicted_Next_30_Days_Amount', ascending=True).head())
    else:
        print("No customers found for inference features. Cannot perform inference.")

if __name__ == "__main__":
    main()
```
:::

**è©•ä¾¡:**
- ãƒ¢ãƒ‡ãƒ«ã«ã¯ Random Forest ãŒä½¿ã‚ã‚Œã¦ã„ã‚‹ï¼ˆLightGBM ã‚’ä½¿ã£ã¦ã»ã—ã„ï¼‰ã€‚
- è©•ä¾¡æŒ‡æ¨™ã¯ MAE, R2 ãŒä½¿ã‚ã‚Œã¦ã„ã‚‹ï¼ˆRMSE ãŒã»ã—ã„ï¼‰ã€‚
- æ¤œè¨¼ã¯å˜ç´”ãª holdout ã§è¡Œã‚ã‚Œã¦ã„ã‚‹ï¼ˆäº¤å·®æ¤œè¨¼ã«ã—ãŸã„ï¼‰ã€‚
- ç‰¹å¾´é‡ã®ä½œæˆã«ãŠã„ã¦ `Quantity` ã‚„ `UnitPrice` ãŒ 0 ã‚ˆã‚Šå¤§ãã„ãƒ¬ã‚³ãƒ¼ãƒ‰ã«çµã£ã¦ã„ã¾ã™ã€‚ã“ã‚Œã¯æ°—ãŒåˆ©ã„ã¦ã„ã‚‹ã¨ã‚‚å–ã‚Œã¾ã™ãŒã€æœ¬æ¥ã¯ãƒ‡ãƒ¼ã‚¿ã®ä»•æ§˜ã‚„ãƒ“ã‚¸ãƒã‚¹è¦ä»¶ã«é–¢ã‚ã‚‹éƒ¨åˆ†ãªã®ã§ã€å‹æ‰‹ã«åˆ¤æ–­ã›ãšç¢ºèªã—ã¦ã»ã—ã„éƒ¨åˆ†ã§ã‚‚ã‚ã‚Šã¾ã™ã€‚

### æ€§èƒ½æ”¹å–„ã®ãŸã‚ã®ä¼šè©±ã¨ä¿®æ­£

æ±åŒ–æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã€äº¤å·®æ¤œè¨¼ã¸ã®å¤‰æ›´ã‚’æŒ‡ç¤ºã—ã¾ã™ã€‚

```
æ±åŒ–æ€§èƒ½ã‚’å‘ä¸Šã•ã›ãŸã„ã§ã™ã€‚ç¾åœ¨ã¯å˜ç´”ãªholdoutã§ã™ãŒã€äº¤å·®æ¤œè¨¼ã«å¤‰æ›´ã—ã¦ãã ã•ã„ã€‚
ã“ã‚Œã¾ã§ã«å¼•ãç¶šãã€ãƒªãƒ¼ã‚¯ã«ã¯ååˆ†ã«æ°—ã‚’ã¤ã‘ã¦ãã ã•ã„ã€‚
```

:::details ç”Ÿæˆã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰ï¼ˆäº¤å·®æ¤œè¨¼ / æŠœç²‹ï¼‰
```python

# Time Series Cross-Validation
n_splits = 4
tscv = TimeSeriesSplit(n_splits=n_splits)

cv_mae_scores = []
cv_r2_scores = []

print(f"\nStarting Time Series Cross-Validation with {n_splits} splits...")

fold = 0
for train_index, val_index in tscv.split(unique_dates):
    fold += 1
    val_dates = unique_dates[val_index]
    potential_train_dates = unique_dates[train_index]
    
    # LEAKAGE PREVENTION:
    # Ensure there is a gap between the training data and validation data.
    # Validation targets are for [val_date, val_date + 29].
    # Training targets are for [train_date, train_date + 29].
    # We must strictly ensure that the Training Target period ends before the Validation Prediction Date?
    # Actually, more strictly: When making a prediction at `val_date`, we only have access to data 
    # where the target outcome is already known.
    # Target outcome for `train_date` is known at `train_date + 30`.
    # So we need `train_date + 30 <= val_date`.
    
    min_val_date = val_dates.min()
    valid_train_dates = [d for d in potential_train_dates if d + pd.Timedelta(days=30) <= min_val_date]
    
    if not valid_train_dates:
        print(f"Fold {fold}: Skipped - Not enough history for training with strict leakage prevention gap.")
        continue
        
    train_df = full_dataset[full_dataset['prediction_date'].isin(valid_train_dates)]
    val_df = full_dataset[full_dataset['prediction_date'].isin(val_dates)]
    
    X_train = train_df.drop(columns=['CustomerID', 'next_30_days_total_amount', 'prediction_date'])
    y_train = train_df['next_30_days_total_amount']

    X_val = val_df.drop(columns=['CustomerID', 'next_30_days_total_amount', 'prediction_date'])
    y_val = val_df['next_30_days_total_amount']
    
    model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)
    model.fit(X_train, y_train)
    
    y_pred = model.predict(X_val)
    
    mae = mean_absolute_error(y_val, y_pred)
    r2 = r2_score(y_val, y_pred)
    
    cv_mae_scores.append(mae)
    cv_r2_scores.append(r2)
    
    print(f"Fold {fold}: Train Dates: {len(valid_train_dates)} ({pd.to_datetime(valid_train_dates[0]).date()} to {pd.to_datetime(valid_train_dates[-1]).date()}), Val Dates: {len(val_dates)} ({pd.to_datetime(val_dates[0]).date()} to {pd.to_datetime(val_dates[-1]).date()}) - MAE: {mae:.2f}, R2: {r2:.2f}")

if cv_mae_scores:
    print(f"\nAverage MAE across {len(cv_mae_scores)} folds: {np.mean(cv_mae_scores):.2f}")
    print(f"Average R2 across {len(cv_mae_scores)} folds: {np.mean(cv_r2_scores):.2f}")
else:
    print("\nNo valid folds found for cross-validation.")
    return

```
:::

ã“ã®å¤‰æ›´ã¯ç‰¹ã«å•é¡Œãªã•ãã†ã§ã™ã€‚

ã•ã‚‰ã«ã€è©•ä¾¡æŒ‡æ¨™ã®å¤‰æ›´ã¨ã€æ¤œè¨¼æœŸé–“ã®åˆ»ã¿ã‚’å¤‰æ›´ã™ã‚‹æŒ‡ç¤ºã‚’è¿½åŠ ã—ã¾ã™ã€‚

```bash
- è©•ä¾¡æŒ‡æ¨™ã«RMSEã‚’è¿½åŠ ã—ã¦
- current_prediction_dateãŒ7æ—¥åˆ»ã¿ã§ã™ãŒ30æ—¥ã”ã¨ã«ã—ã¦
```

å¤‰æ›´ãŒè¤‡æ•°ç®‡æ‰€ã«ã‚ãŸã‚‹ã®ã§ã‚³ãƒ¼ãƒ‰ã¯çœç•¥ã—ã¾ã™ãŒã€ã“ã‚Œã«ã‚ˆã‚Šãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆã®é–“éš”ãŒ30æ—¥ã«ãªã‚Šã€RMSEãŒå‡ºåŠ›ã•ã‚Œã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚ãŸã ã€ã“ã†ã„ã£ãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å¤‰æ›´ã¯æœ¬æ¥ã¯ã‚ã–ã‚ã–æŒ‡ç¤ºã‚’å‡ºã•ãšã¨ã‚‚è‡ªåˆ†ã§ã‚³ãƒ¼ãƒ‰ã‚’ä¿®æ­£ã—ãŸã»ã†ãŒæ—©ã„ã§ã™ã€‚
æ¬¡ã«ã€ãƒ¢ãƒ‡ãƒ«ã‚’ LightGBM ã«å¤‰æ›´ã—ã¾ã™ã€‚

```docker
ç²¾åº¦ã‚’æ”¹å–„ã—ãŸã„ã®ã§ãƒ¢ãƒ‡ãƒ«ã‚’LightGBMã«å¤‰æ›´ã—ãŸã„ã§ã™ã€‚
scikit-learnãƒ©ãƒƒãƒ‘ãƒ¼ã§ã¯ãªãå…ƒã®LightGBMã®Python APIã‚’ç”¨ã„ã¦ãã ã•ã„ã€‚
```

äºˆæ¸¬çµæœã®è§£é‡ˆæ€§å‘ä¸Šã®ãŸã‚ã€ç‰¹å¾´é‡é‡è¦åº¦ã‚’å‡ºåŠ›ã•ã›ã¾ã™ã€‚

```python
äºˆæ¸¬çµæœã‚’è§£é‡ˆã—ã‚„ã™ãã—ãŸã„ã®ã§importanceã‚’å‡ºåŠ›ã—ã¦
```

ã“ã“ã¾ã§ã®ã‚³ãƒ¼ãƒ‰ã¯ã“ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚å¤§ããªå•é¡Œã¯ã‚ã‚Šã¾ã›ã‚“ãŒã€é‡è¦åº¦ã¯ã‚°ãƒ©ãƒ•ã§å‡ºåŠ›ã—ã¦ã»ã—ã„ãªãƒ¼ã¨ã‚‚æ€ã„ã¾ã™ã€‚ã—ã‹ã—ã“ã®ã‚ãŸã‚Šã‹ã‚‰ä¼šè©±ã«ç–²ã‚Œã¦ããŸãŸã‚æŒ‡ç¤ºã‚’çœã„ã¦ã—ã¾ã„ã¾ã—ãŸã€‚

:::details ç”Ÿæˆã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰ï¼ˆLightGBM, ç‰¹å¾´é‡é‡è¦åº¦ã®è¿½åŠ ã¾ã§ï¼‰
```
# Create LightGBM datasets
dtrain = lgb.Dataset(X_train, label=y_train)
dval = lgb.Dataset(X_val, label=y_val, reference=dtrain)

# Train model
model = lgb.train(
    params,
    dtrain,
    num_boost_round=1000,
    valid_sets=[dtrain, dval],
    callbacks=[lgb.early_stopping(stopping_rounds=50), lgb.log_evaluation(period=0)] # period=0 disables verbose logging
)

best_iterations.append(model.best_iteration)

y_pred = model.predict(X_val, num_iteration=model.best_iteration)

# (ä¸­ç•¥..)

if not X_inference.empty:
    inference_predictions = model_final.predict(X_inference, num_iteration=model_final.best_iteration)
    
    inference_results = pd.DataFrame({
        'CustomerID': inference_features_df['CustomerID'],
        'Predicted_Next_30_Days_Amount': inference_predictions
    })
    
    print("\nTop 5 customers with highest predicted purchase amount for the next 30 days:")
    print(inference_results.sort_values(by='Predicted_Next_30_Days_Amount', ascending=False).head())

    print("\nBottom 5 customers with lowest (or zero) predicted purchase amount for the next 30 days:")
    print(inference_results.sort_values(by='Predicted_Next_30_Days_Amount', ascending=True).head())
else:
    print("No customers found for inference features. Cannot perform inference.")

```
:::


ç‰¹å¾´é‡ã«é‡è¦åº¦ã‚’å‡ºåŠ›ã—ãŸã¨ã“ã‚ã§ã€ç‰¹å¾´é‡ãŒ3ã¤ã—ã‹ä½¿ã‚ã‚Œã¦ã„ãªã„ã“ã¨ã«æ°—ã¥ãã¾ã—ãŸã€‚
ç²¾åº¦æ”¹å–„ã®ãŸã‚ã€ç‰¹å¾´é‡ã®è¿½åŠ ã‚’å…·ä½“çš„ã«æŒ‡ç¤ºã—ã¾ã™ã€‚

```docker
ã•ã‚‰ã«ç²¾åº¦ã‚’æ”¹å–„ã™ã‚‹ãŸã‚ã«ç‰¹å¾´é‡è¿½åŠ ã‚’è©¦ã—ã¾ã™ã€‚
ä»¥ä¸‹ã®ç‰¹å¾´é‡ã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚
- ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°
- InvoiceNoã®nunique
- StockCodeã®nunique
- InvoiceDateã®nunique
- countryã®mode, nunique
- ã“ã‚Œã¾ã§ã®è³¼å…¥é‡‘é¡ã®sum, max, min, median
```

ã“ã®æŒ‡ç¤ºã«å¯¾ã—ã¦ã€æœ€åˆã¯é›†è¨ˆæœŸé–“ã‚’å›ºå®šã—ãªã„éå»é€šç®—ã®é›†è¨ˆã‚’è¿½åŠ ã—ã¦ããŸã‚Šã€å¾®å¦™ãªä¿®æ­£ã‚’ä½•åº¦ã‹ç¹°ã‚Šè¿”ã™å¿…è¦ãŒã‚ã‚Šã¾ã—ãŸã€‚ä¾‹ãˆã°ã€ã“ã“ã¾ã§30æ—¥windowã§é›†è¨ˆã—ã¦ã„ãŸã®ã«æ€¥ã«180æ—¥ã®é›†è¨ˆãŒåŠ ã‚ã‚Šã€ã“ã®ä¿®æ­£ã®ãŸã‚ã«2,3å¾€å¾©ã‚’è¦ã—ã¾ã—ãŸã€‚
é›†è¨ˆãƒ­ã‚¸ãƒƒã‚¯ã®ã‚ˆã†ãªç´°ã‹ã„éƒ¨åˆ†ã¯ã€è¨€è‘‰ã§æŒ‡ç¤ºã™ã‚‹ã‚ˆã‚Šã‚‚è‡ªåˆ†ã§æ›¸ãã‹äº‹å‰ã«è©³ç´°ãªä»•æ§˜ã‚’ä¼ãˆã‚‹æ–¹ãŒç¢ºå®Ÿã ã¨æ„Ÿã˜ã¾ã—ãŸã€‚

:::details ä¿®æ­£ã®éç¨‹ï¼ˆæŠœç²‹ï¼‰
```python
+ long_window_features = long_window_df.groupby('CustomerID').agg(
+     last_180_days_record_count=('InvoiceNo', 'count'),
      # (ä¸­ç•¥...)
+     last_180_days_amount_median=('Amount', 'median')
+ ).reset_index()
```
:::


### æœ€çµ‚çš„ãªã‚³ãƒ¼ãƒ‰

ä¼šè©±ã‚’ç¶šã‘ã¦å®Œæˆã—ãŸã‚³ãƒ¼ãƒ‰ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ã€‚ã“ã“ã¾ã§ã§ç­†è€…ã®ã‚„ã‚ŠãŸã„ã“ã¨ï¼ˆå‰è¨˜äº‹ã®è©³ç´°ãªè¦ä»¶ï¼‰ã¯æ¦‚ã­å…¥ã£ã¦ã¯ã„ã‚‹ã®ã§ã™ãŒã€å¾®å¦™ã«å†—é•·ã ã£ãŸã‚Šè¨­å®šã‚’å¤‰æ›´ã—ãŸã„ç®‡æ‰€ãŒå°‘ãªã‹ã‚‰ãšã‚ã‚Šã€ä¼šè©±ã ã‘ã§ã®ç·¨é›†ã¯ã¾ã©ã‚ã£ã“ã—ã•ãŒã‚ã‚‹ã¨æ„Ÿã˜ã¾ã—ãŸã€‚ä¼šè©±ä¸»ä½“ã§ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã™ã‚‹å ´åˆã§ã‚‚ç›´æ¥ç·¨é›†ã¯ä½µç”¨ã—ãŸã„ã§ã™ã­ã€‚ï¼ˆãã‚Œã¯ãã†ï¼‰

:::details æœ€çµ‚çš„ãªã‚³ãƒ¼ãƒ‰ï¼ˆã‚„ã‚ŠãŸã„ã“ã¨ãŒæ±ºã¾ã£ã¦ã„ã‚‹å ´åˆï¼‰
```python
import lightgbm as lgb
import numpy as np
import pandas as pd
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import TimeSeriesSplit

def preprocess_data(df):
    df = df.copy()  # Operate on a copy to avoid SettingWithCopyWarning
    # Convert InvoiceDate to datetime
    df["InvoiceDate"] = pd.to_datetime(df["InvoiceDate"])

    # Calculate Amount
    df["Amount"] = df["Quantity"] * df["UnitPrice"]

    # Drop rows with missing CustomerID
    df = df.dropna(subset=["CustomerID"])
    df["CustomerID"] = df["CustomerID"].astype(int)

    # Filter out returns (negative Quantity) and invalid prices/quantities
    df = df[(df["Quantity"] > 0) & (df["UnitPrice"] > 0)]

    # Convert Country to category for LightGBM
    df["Country"] = df["Country"].astype("category")

    # Sort by CustomerID and InvoiceDate for time-series operations
    df = df.sort_values(by=["CustomerID", "InvoiceDate"])

    return df

def create_features_and_target(df, prediction_date):
    # Data for short feature window (past 30 days relative to prediction_date)
    short_window_days = 30
    feature_start_date = prediction_date - pd.Timedelta(days=short_window_days)
    feature_end_date = prediction_date - pd.Timedelta(days=1)

    # Data for target window (next 30 days from prediction_date)
    target_start_date = prediction_date
    target_end_date = prediction_date + pd.Timedelta(days=29)

    # Define a helper for mode that handles empty/multiple values
    def get_mode(x):
        m = x.mode()
        if not m.empty:
            return m.iloc[0]
        return None

    # All Features are now Short Window (Past 30 Days)
    features_df = df[
        (df["InvoiceDate"] >= feature_start_date)
        & (df["InvoiceDate"] <= feature_end_date)
    ]
    customer_features = (
        features_df.groupby("CustomerID")
        .agg(
            # Existing 30-day features
            last_30_days_total_amount=("Amount", "sum"),
            last_30_days_order_count=("InvoiceNo", "nunique"),
            last_30_days_item_count=("Quantity", "sum"),
            # Newly requested features, now also for last 30 days
            last_30_days_record_count=("InvoiceNo", "count"),
            last_30_days_stock_nunique=("StockCode", "nunique"),
            last_30_days_invoicedate_nunique=("InvoiceDate", "nunique"),
            last_30_days_country_mode=("Country", get_mode),
            last_30_days_country_nunique=("Country", "nunique"),
            last_30_days_amount_max=("Amount", "max"),
            last_30_days_amount_min=("Amount", "min"),
            last_30_days_amount_median=("Amount", "median"),
        )
        .reset_index()
    )

    # Target: aggregate next 30 days purchases for each customer
    target_df = df[
        (df["InvoiceDate"] >= target_start_date)
        & (df["InvoiceDate"] <= target_end_date)
    ]
    customer_target = (
        target_df.groupby("CustomerID")
        .agg(next_30_days_total_amount=("Amount", "sum"))
        .reset_index()
    )

    # Merge features and target
    # Base customers should ideally be those active in the feature window or target window
    all_customer_ids = pd.concat(
        [customer_features["CustomerID"], customer_target["CustomerID"]]
    ).unique()

    # Create a base dataframe with all relevant CustomerIDs for this prediction_date
    base_customers_df = pd.DataFrame({"CustomerID": all_customer_ids})

    merged_df = pd.merge(
        base_customers_df, customer_features, on="CustomerID", how="left"
    )
    merged_df = pd.merge(merged_df, customer_target, on="CustomerID", how="left")

    # Fill NaN targets with 0
    merged_df["next_30_days_total_amount"] = merged_df[
        "next_30_days_total_amount"
    ].fillna(0)

    # Fill NaN features
    # Numeric features: fill with 0
    numeric_cols = [
        "last_30_days_total_amount",
        "last_30_days_order_count",
        "last_30_days_item_count",
        "last_30_days_record_count",
        "last_30_days_stock_nunique",
        "last_30_days_invoicedate_nunique",
        "last_30_days_country_nunique",
        "last_30_days_amount_max",
        "last_30_days_amount_min",
        "last_30_days_amount_median",
    ]
    merged_df[numeric_cols] = merged_df[numeric_cols].fillna(0)

    # Categorical feature (Country mode): Ensure dtype is category
    merged_df["last_30_days_country_mode"] = merged_df[
        "last_30_days_country_mode"
    ].astype("category")

    return merged_df

def main():
    print("Loading data...")
    df = pd.read_csv("data.csv", encoding="latin1")
    print("Data loaded.")

    print("Preprocessing data...")
    df = preprocess_data(df)
    print("Data preprocessed.")

    min_date = df["InvoiceDate"].min()
    max_date = df["InvoiceDate"].max()
    print(f"Date range: {min_date} to {max_date}")

    total_days = (max_date - min_date).days

    # Generate samples for training and validation
    all_data_points = []

    current_prediction_date = min_date + pd.Timedelta(
        days=60
    )  # Start generating samples from here

    # Ensure that current_prediction_date + 29 days (for target window) does not exceed max_date
    while current_prediction_date + pd.Timedelta(days=29) <= max_date:
        features_target = create_features_and_target(df, current_prediction_date)

        # Add the prediction date to the features_target df for splitting
        features_target["prediction_date"] = current_prediction_date
        all_data_points.append(features_target)

        current_prediction_date += pd.Timedelta(
            days=30
        )  # Move prediction window by 30 days

    full_dataset = pd.concat(all_data_points, ignore_index=True)

    # Sort by prediction date to ensure correct time-based splitting logic
    full_dataset.sort_values("prediction_date", inplace=True)

    unique_dates = np.sort(full_dataset["prediction_date"].unique())
    print(f"Total generated samples: {len(full_dataset)}")
    print(f"Unique prediction dates: {len(unique_dates)}")

    # Time Series Cross-Validation
    n_splits = 4
    tscv = TimeSeriesSplit(n_splits=n_splits)

    cv_mae_scores = []
    cv_r2_scores = []
    cv_rmse_scores = []
    best_iterations = []

    print(f"\nStarting Time Series Cross-Validation with {n_splits} splits...")

    params = {
        "objective": "regression",
        "metric": "rmse",
        "boosting_type": "gbdt",
        "verbosity": -1,
        "seed": 42,
        "num_leaves": 31,
        "learning_rate": 0.05,
        "feature_fraction": 0.9,
    }

    fold = 0
    for train_index, val_index in tscv.split(unique_dates):
        fold += 1
        val_dates = unique_dates[val_index]
        potential_train_dates = unique_dates[train_index]

        # LEAKAGE PREVENTION:
        min_val_date = val_dates.min()
        valid_train_dates = [
            d
            for d in potential_train_dates
            if d + pd.Timedelta(days=30) <= min_val_date
        ]

        if not valid_train_dates:
            print(
                f"Fold {fold}: Skipped - Not enough history for training with strict leakage prevention gap."
            )
            continue

        train_df = full_dataset[full_dataset["prediction_date"].isin(valid_train_dates)]
        val_df = full_dataset[full_dataset["prediction_date"].isin(val_dates)]

        X_train = train_df.drop(
            columns=["CustomerID", "next_30_days_total_amount", "prediction_date"]
        )
        y_train = train_df["next_30_days_total_amount"]

        X_val = val_df.drop(
            columns=["CustomerID", "next_30_days_total_amount", "prediction_date"]
        )
        y_val = val_df["next_30_days_total_amount"]

        # Create LightGBM datasets
        dtrain = lgb.Dataset(X_train, label=y_train)
        dval = lgb.Dataset(X_val, label=y_val, reference=dtrain)

        # Train model
        model = lgb.train(
            params,
            dtrain,
            num_boost_round=1000,
            valid_sets=[dtrain, dval],
            callbacks=[
                lgb.early_stopping(stopping_rounds=50),
                lgb.log_evaluation(period=0), # period=0 disables verbose logging
            ],
        )

        best_iterations.append(model.best_iteration)

        y_pred = model.predict(X_val, num_iteration=model.best_iteration)

        mae = mean_absolute_error(y_val, y_pred)
        mse = mean_squared_error(y_val, y_pred)
        rmse = np.sqrt(mse)
        r2 = r2_score(y_val, y_pred)

        cv_mae_scores.append(mae)
        cv_r2_scores.append(r2)
        cv_rmse_scores.append(rmse)

        print(
            f"Fold {fold}: Train Dates: {len(valid_train_dates)}, Val Dates: {len(val_dates)} - MAE: {mae:.2f}, RMSE: {rmse:.2f}, R2: {r2:.2f}, Best Iter: {model.best_iteration}"
        )

    if cv_mae_scores:
        print(
            f"\nAverage MAE across {len(cv_mae_scores)} folds: {np.mean(cv_mae_scores):.2f}"
        )
        print(
            f"Average RMSE across {len(cv_rmse_scores)} folds: {np.mean(cv_rmse_scores):.2f}"
        )
        print(
            f"Average R2 across {len(cv_mae_scores)} folds: {np.mean(cv_r2_scores):.2f}"
        )
        avg_best_iter = int(np.mean(best_iterations))
        print(f"Average best iteration: {avg_best_iter}")
    else:
        print("\nNo valid folds found for cross-validation.")
        return

    # --- Final Retraining for Inference ---
    print("\nRetraining model on all available data for future inference...")
    X_final_train = full_dataset.drop(
        columns=["CustomerID", "next_30_days_total_amount", "prediction_date"]
    )
    y_final_train = full_dataset["next_30_days_total_amount"]

    dtrain_final = lgb.Dataset(X_final_train, label=y_final_train)

    # Train with the average best iteration found in CV
    model_final = lgb.train(
        params,
        dtrain_final,
        num_boost_round=avg_best_iter,
        valid_sets=[dtrain_final],
        callbacks=[lgb.log_evaluation(period=0)],
    )
    print("Final model trained.")

    # --- Feature Importance ---
    print("\n--- Feature Importances ---")
    feature_importances = model_final.feature_importance()
    feature_names = model_final.feature_name()

    importance_df = pd.DataFrame(
        {"Feature": feature_names, "Importance": feature_importances}
    )
    importance_df = importance_df.sort_values(by="Importance", ascending=False)

    print("Top 20 most important features:")
    print(importance_df.head(20))

    # --- Inference Example ---
    inference_prediction_date = max_date + pd.Timedelta(days=1)
    print(
        f"\n--- Performing Inference for a future prediction date: {inference_prediction_date} ---"
    )

    inference_features_df = create_features_and_target(df, inference_prediction_date)

    X_inference = inference_features_df.drop(
        columns=["CustomerID", "next_30_days_total_amount"]
    )

    if not X_inference.empty:
        inference_predictions = model_final.predict(
            X_inference, num_iteration=model_final.best_iteration
        )

        inference_results = pd.DataFrame(
            {
                "CustomerID": inference_features_df["CustomerID"],
                "Predicted_Next_30_Days_Amount": inference_predictions,
            }
        )

        print(
            "\nTop 5 customers with highest predicted purchase amount for the next 30 days:"
        )
        print(
            inference_results.sort_values(
                by="Predicted_Next_30_Days_Amount", ascending=False
            ).head()
        )

        print(
            "\nBottom 5 customers with lowest (or zero) predicted purchase amount for the next 30 days:"
        )
        print(
            inference_results.sort_values(
                by="Predicted_Next_30_Days_Amount", ascending=True
            ).head()
        )
    else:
        print("No customers found for inference features. Cannot perform inference.")

if __name__ == "__main__":
    main()
```
:::

### æ„Ÿæƒ³

- ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å¤‰æ›´ãªã©ã®ç´°ã‹ã„ä¿®æ­£ã¯ã€ç›´æ¥ã‚³ãƒ¼ãƒ‰ã‚’ç·¨é›†ã™ã‚‹æ–¹ãŒæ˜ã‚‰ã‹ã«æ—©ã„ã§ã™ã€‚
- äº¤å·®æ¤œè¨¼ã®å®Ÿè£…ãªã©ã€ã‚„ã‚‹ã“ã¨ã¯æ˜ç¢ºã ãŒå®Ÿè£…é‡ãŒåœ°å‘³ã«å¤šã„ã€ã¨ã„ã†ã‚ˆã†ãªå¤‰æ›´ã‚’ä»»ã›ã‚‹ã®ã¯åŠ¹ç‡çš„ã ã¨æ„Ÿã˜ã¾ã—ãŸã€‚
- ä»•æ§˜ãŒæ›–æ˜§ãªéƒ¨åˆ†ã§å‹æ‰‹ãªè§£é‡ˆãŒå…¥ã‚‹ã“ã¨ãŒã‚ã‚‹ãŸã‚ã€æ˜ç¢ºãªæŒ‡ç¤ºãŒãªã„ç®‡æ‰€ã¯æ³¨æ„ãŒå¿…è¦ã§ã™ã€‚
- ç”Ÿæˆã•ã‚Œã‚‹ãŸã³ã«ãƒ¬ãƒ“ãƒ¥ãƒ¼ãŒå¿…è¦ã§ã™ãŒã€é »ç¹ãªãƒ¬ãƒ“ãƒ¥ãƒ¼ã¯æ³¨æ„åŠ›ã‚’è¦ã™ã‚‹ã®ã¨ã€å°‘ã—ã®æ²¹æ–­ã§ãƒã‚°ã‚„ä»•æ§˜ã®å‹˜é•ã„ã‚’è¦‹è½ã¨ã—å¾—ã‚‹ã¨æ„Ÿã˜ã¾ã—ãŸã€‚ï¼ˆã¨ã„ã†ã‹åº¦ã€…è¦‹è½ã¨ã—ã¦ã„ãŸï¼‰


## 2. å£æ‰“ã¡ã—ãªãŒã‚‰å®Ÿè£…ã‚’é€²ã‚ã‚‹å ´åˆ

æ¬¡ã¯å…·ä½“çš„ãªæ”¹å–„æ¡ˆã‚’æŒãŸãšã«ã€å®Ÿè£…æ–¹é‡ã‚‚å«ã‚ã¦ LLM ã«ç›¸è«‡ã—ãªãŒã‚‰é€²ã‚ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã™ã€‚
ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã‚„æ¨è«–ã®æ–‡è„ˆã§ã¯ã€ç­†è€…è‡ªèº«ã¯ã“ã®æ–¹æ³•ã¯ã‚ã¾ã‚Šä½¿ã‚ãªã„ã®ã§ã™ãŒã€å®Ÿè£…ã™ã‚‹è¨€èªã‚„å†…å®¹ãŒä¸æ…£ã‚Œãªå ´åˆï¼ˆç­†è€…ã®å ´åˆã¯ä»–åˆ†é‡ã®ã‚¢ãƒ—ãƒªå®Ÿè£…ãªã©ï¼‰ã«ã“ã†ã„ã£ãŸé€²ã‚æ–¹ã‚’ã™ã‚‹ã“ã¨ãŒã‚ã‚‹ã¨è€ƒãˆã‚‰ã‚Œã‚‹ãŸã‚ã€å‚è€ƒã®ãŸã‚ç¢ºèªã—ã¦ãŠãã¾ã™ã€‚
ã“ã“ã§ã¯ã€LLM ãŒè¨€ã£ãŸã“ã¨ã¯åŸºæœ¬çš„ã«ä¿¡ç”¨ã—ã¦ã€å®Ÿè£…ã‚’é€²ã‚ã‚‹ã“ã¨ã«ã—ã¾ã™ã€‚

### åˆæœŸç”Ÿæˆã¨å‰å‡¦ç†ã®ç›¸è«‡

ä½•ã¯ã¨ã‚‚ã‚ã‚Œã¾ãšã¯åˆæœŸã‚³ãƒ¼ãƒ‰ã‚’ç”Ÿæˆã—ã¾ã™ã€‚

```
ã¾ãš @05_wallball_requirements.txt ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚
ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãƒ™ãƒ¼ã‚¹ã«ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã€æ¨è«–ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚
```
:::details ç”Ÿæˆã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰
```
# /// script
# dependencies = ["pandas", "scikit-learn", "numpy"]
# ///
"""
Purchase Amount Prediction Model

This script implements a machine learning pipeline to predict the purchase amount 
of customers for the next 30 days based on their activity in the past 30 days.

Key Features:
- Data: 'data.csv' (Online Retail Dataset)
- Preprocessing: Cleaning missing IDs, date conversion, total amount calculation.
- Feature Engineering:
    - Aggregated by CustomerID.
    - Window: [Prediction Date - 30 days, Prediction Date).
    - Features: Monetary (Sum), Frequency (Count), Recency (Days since last purchase).
- Target:
    - Window: [Prediction Date, Prediction Date + 30 days).
    - Value: Sum of purchase amount.
- Validation:
    - Time-series split to avoid leakage.
    - Train data: Multiple 30-day windows sliding back from the test period.
    - Test data: The most recent 30-day window with available ground truth.
- Model: Random Forest Regressor.

Usage:
    uv run 05_wallball.py
"""
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error
import datetime

def load_and_preprocess(filepath):
    print("Loading data...")
    # Use specific encoding as detected in exploration
    df = pd.read_csv(filepath, encoding='ISO-8859-1')
    
    # Preprocessing
    print("Preprocessing...")
    # Drop rows with missing CustomerID
    df = df.dropna(subset=['CustomerID'])
    
    # Convert CustomerID to int (it's usually float when loaded with NaNs)
    df['CustomerID'] = df['CustomerID'].astype(int)
    
    # Convert InvoiceDate to datetime
    df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])
    
    # Calculate TotalAmount
    df['TotalAmount'] = df['Quantity'] * df['UnitPrice']
    
    # Remove extremely weird dates if any (sanity check not strictly needed if pd.to_datetime works)
    return df

def get_features_and_target(df, cutoff_date, known_customers):
    """
    Generates features and target for a specific cutoff date.
    Features: 30 days before cutoff.
    Target: 30 days after cutoff.
    """
    feature_start = cutoff_date - datetime.timedelta(days=30)
    target_end = cutoff_date + datetime.timedelta(days=30)
    
    # Data for features
    df_features = df[(df['InvoiceDate'] >= feature_start) & (df['InvoiceDate'] < cutoff_date)]
    
    # Data for target
    df_target = df[(df['InvoiceDate'] >= cutoff_date) & (df['InvoiceDate'] < target_end)]
    
    # Initialize result dataframe with all known customers
    # This ensures we predict for inactive customers too (who might purchase 0)
    res = pd.DataFrame({'CustomerID': list(known_customers)})
    
    # --- Feature Engineering ---
    # Aggregate feature data
    # 1. Monetary: Sum of TotalAmount
    # 2. Frequency: Count of unique InvoiceNo
    # 3. Recency: Days since last purchase (relative to cutoff)
    
    agg_dict = {
        'TotalAmount': 'sum',
        'InvoiceNo': 'nunique',
        'InvoiceDate': 'max'
    }
    
    feat_agg = df_features.groupby('CustomerID').agg(agg_dict).reset_index()
    feat_agg.rename(columns={
        'TotalAmount': 'history_monetary',
        'InvoiceNo': 'history_frequency',
        'InvoiceDate': 'last_purchase_date'
    }, inplace=True)
    
    # Merge features
    res = res.merge(feat_agg, on='CustomerID', how='left')
    
    # Fill NaNs for customers with no activity in feature window
    res['history_monetary'] = res['history_monetary'].fillna(0)
    res['history_frequency'] = res['history_frequency'].fillna(0)
    
    # Calculate Recency
    # If no purchase in window, we can set it to 30 (max window size) or a distinct value
    # Since we only look at past 30 days, if last_purchase_date is null, it means > 30 days.
    res['history_recency'] = (cutoff_date - res['last_purchase_date']).dt.days
    res['history_recency'] = res['history_recency'].fillna(30) # Fill missing with window size
    
    # Drop helper column
    res = res.drop(columns=['last_purchase_date'])
    
    # --- Target Generation ---
    target_agg = df_target.groupby('CustomerID')['TotalAmount'].sum().reset_index()
    target_agg.rename(columns={'TotalAmount': 'target_amount'}, inplace=True)
    
    # Merge target
    res = res.merge(target_agg, on='CustomerID', how='left')
    res['target_amount'] = res['target_amount'].fillna(0)
    
    return res

def main():
    filepath = 'data.csv'
    df = load_and_preprocess(filepath)
    
    print(f"Data range: {df['InvoiceDate'].min()} to {df['InvoiceDate'].max()}")
    
    max_date = df['InvoiceDate'].max()
    
    # Define Test Cutoff (Leave 30 days for target)
    test_cutoff = max_date - datetime.timedelta(days=30)
    # Adjust to start of day to be clean? Or keep exact time?
    # Let's align cutoffs to 00:00:00 for consistency
    test_cutoff = test_cutoff.replace(hour=0, minute=0, second=0, microsecond=0)
    
    print(f"Test Cutoff: {test_cutoff}")
    
    # Define Training Cutoffs
    # We'll create a few slices for training to capture seasonality/trends
    # e.g., monthly back from test_cutoff
    train_cutoffs = []
    current_cutoff = test_cutoff - datetime.timedelta(days=30)
    for _ in range(6): # Take 6 months of history for training samples
        if current_cutoff < df['InvoiceDate'].min() + datetime.timedelta(days=30):
            break
        train_cutoffs.append(current_cutoff)
        current_cutoff -= datetime.timedelta(days=30)
        
    print(f"Training Cutoffs: {train_cutoffs}")
    
    train_dfs = []
    
    # Collect all customers known before the split to avoid lookahead bias in "who exists"
    # But simplistic approach: "All customers seen in dataset up to cutoff"
    
    for cutoff in train_cutoffs:
        print(f"Generating training data for cutoff: {cutoff}")
        # Customers known up to this cutoff
        known_mask = df['InvoiceDate'] < cutoff
        known_customers = df.loc[known_mask, 'CustomerID'].unique()
        
        train_chunk = get_features_and_target(df, cutoff, known_customers)
        train_dfs.append(train_chunk)
        
    train_data = pd.concat(train_dfs, ignore_index=True)
    
    print(f"Generating test data for cutoff: {test_cutoff}")
    known_customers_test = df.loc[df['InvoiceDate'] < test_cutoff, 'CustomerID'].unique()
    test_data = get_features_and_target(df, test_cutoff, known_customers_test)
    
    print(f"Train shape: {train_data.shape}")
    print(f"Test shape: {test_data.shape}")
    
    # Features and Target
    features = ['history_monetary', 'history_frequency', 'history_recency']
    target = 'target_amount'
    
    X_train = train_data[features]
    y_train = train_data[target]
    
    X_test = test_data[features]
    y_test = test_data[target]
    
    # Model Training
    print("Training Random Forest...")
    rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)
    rf.fit(X_train, y_train)
    
    # Prediction
    preds = rf.predict(X_test)
    
    # Evaluation
    rmse = np.sqrt(mean_squared_error(y_test, preds))
    mae = mean_absolute_error(y_test, preds)
    
    print("--- Evaluation Results ---")
    print(f"RMSE: {rmse:.4f}")
    print(f"MAE: {mae:.4f}")
    
    # Feature Importance
    importances = pd.Series(rf.feature_importances_, index=features)
    print("\nFeature Importances:")
    print(importances)

    # Optional: Save predictions
    # output = pd.DataFrame({'CustomerID': test_data['CustomerID'], 'Actual': y_test, 'Predicted': preds})
    # output.to_csv('predictions.csv', index=False)

if __name__ == "__main__":
    main()
```
:::

ã“ã®ã‚³ãƒ¼ãƒ‰ã¯ã“ã‚Œã¾ã§ã®æ¦‚è¦æŒ‡ç¤ºã®ã¿ã®ä¸€ç™ºå‡ºã—ã®ã‚³ãƒ¼ãƒ‰ã¨ã»ã¼åŒç­‰ã®å†…å®¹ãªã®ã§ã€ç¶šã‘ã¦ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ã®ç›¸è«‡ã«ç§»ã‚Šã¾ã™ã€‚

```python
ãƒ‡ãƒ¼ã‚¿å†…å®¹ã‚„ç‰¹æ€§ã«ã‚‚ã¨ã¥ã„ã¦å‰å‡¦ç†ã‚’è¡Œã„ãŸã„ã€å¿…è¦ãªå‰å‡¦ç†ã‚’è€ƒãˆã‚‹ãŸã‚ã«ã©ã‚“ãªã“ã¨ã‚’ç¢ºèªã™ã‚Œã°è‰¯ã„ã§ã™ã‹
```

:::details è¿”ç­”
```
âœ¦ ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèªã—ãŸçµæœã€ä»¥ä¸‹ã®å‰å‡¦ç†ã‚’æ¤œè¨ãƒ»å®Ÿè¡Œã™ã¹ãã“ã¨ãŒã‚ã‹ã‚Šã¾ã—ãŸã€‚

  1. CustomerID æ¬ æã®é™¤å¤–
   * ç¾çŠ¶: 135,080ä»¶ï¼ˆå…¨ä½“ã®ç´„25%ï¼‰ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã§ CustomerID ãŒæ¬ æã—ã¦ã„ã¾ã™ã€‚
   * å¯¾å¿œ: é¡§å®¢ã”ã¨ã®è³¼è²·äºˆæ¸¬ã‚’è¡Œã†ã¨ã„ã†ç›®çš„è¨­å®šä¸Šã€é¡§å®¢IDãŒä¸æ˜ãªãƒ‡ãƒ¼ã‚¿ã¯ç´ä»˜ã‘ã‚ˆã†ãŒãªã„ãŸã‚é™¤å¤–ã—ã¾ã™ã€‚

  2. ã‚­ãƒ£ãƒ³ã‚»ãƒ«ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç† (Negative Quantity / InvoiceNo 'C')
   * ç¾çŠ¶: Quantity ãŒè² ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ãŒ 10,624ä»¶ã‚ã‚Šã¾ã™ã€‚ã¾ãŸã€InvoiceNoãŒ 'C' ã§å§‹ã¾ã‚‹ï¼ˆã‚­ãƒ£ãƒ³ã‚»ãƒ«ï¼‰ãƒ¬ã‚³ãƒ¼ãƒ‰ãŒ
     9,288ä»¶ã‚ã‚Šã¾ã™ã€‚
   * å¯¾å¿œ:
       * å˜ç´”ãªäºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã§ã¯ã€ã‚­ãƒ£ãƒ³ã‚»ãƒ«ï¼ˆè² ã®å£²ä¸Šï¼‰ã¯ãƒã‚¤ã‚ºã«ãªã‚Šã‚„ã™ã„ãŸã‚é™¤å¤–ã™ã‚‹ã®ãŒä¸€èˆ¬çš„ã§ã™ã€‚
       * ãŸã ã—ã€ã€Œè¿”å“ãŒå¤šã„é¡§å®¢ã¯å°†æ¥ã®è³¼å…¥é¡ãŒä½ã„ã€ã¨ã„ã£ãŸç‰¹å¾´é‡ã¨ã—ã¦ä½¿ã†ã“ã¨ã‚‚å¯èƒ½ã§ã™ãŒã€ä»Šå›ã¯ãƒ™ãƒ¼ã‚¹
         ãƒ©ã‚¤ãƒ³ã¨ã—ã¦ã‚­ãƒ£ãƒ³ã‚»ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼ˆQuantity < 0ï¼‰ã¯é™¤å¤–ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚

  3. ç‰¹æ®Šãªå–å¼•ã®é™¤å¤– (StockCode / UnitPrice)
   * StockCode: POST (é€æ–™), D (Discount), BANK CHARGES, AMAZONFEE
     ãªã©ã®ç‰¹æ®Šã‚³ãƒ¼ãƒ‰ãŒç¢ºèªã§ãã¾ã—ãŸã€‚ã“ã‚Œã‚‰ã¯å•†å“ã®è³¼å…¥è¡Œå‹•ã¨ã¯ç•°ãªã‚‹ãŸã‚ã€ç´”ç²‹ãªå•†å“è³¼è²·äºˆæ¸¬ã‚’è¡Œã†ãªã‚‰é™¤å¤–
     ã™ã¹ãã§ã™ã€‚
   * UnitPrice: 0ä»¥ä¸‹ã®ä¾¡æ ¼ãŒ2,517ä»¶ã‚ã‚Šã€ãã®å¤šãã¯ Description
     ã‚‚æ¬ æã—ã¦ã„ã¾ã™ã€‚ã“ã‚Œã‚‰ã‚‚æœ‰åŠ¹ãªè³¼è²·è¨˜éŒ²ã§ã¯ãªã„å¯èƒ½æ€§ãŒé«˜ã„ãŸã‚é™¤å¤–ã—ã¾ã™ã€‚

  4. é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã®å‰Šé™¤
   * ç¾çŠ¶: 5,268ä»¶ã®å®Œå…¨é‡è¤‡è¡ŒãŒã‚ã‚Šã¾ã™ã€‚
   * å¯¾å¿œ: é›†è¨ˆçµæœã‚’æ­ªã‚ã‚‹ãŸã‚å‰Šé™¤ã—ã¾ã™ã€‚

  æ¨å¥¨ã•ã‚Œã‚‹å‰å‡¦ç†ã‚³ãƒ¼ãƒ‰ã®å¤‰æ›´æ¡ˆ

  ã“ã‚Œã‚‰ã®å‡¦ç†ã‚’ 05_wallball.py ã® load_and_preprocess é–¢æ•°ã«è¿½åŠ å®Ÿè£…ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚

    1     # ... (èª­ã¿è¾¼ã¿å¾Œ) ...
    2 
    3     # 1. é‡è¤‡å‰Šé™¤
    4     df = df.drop_duplicates()
    5 
    6     # 2. CustomerIDæ¬ æå‰Šé™¤
    7     df = df.dropna(subset=['CustomerID'])
    8     df['CustomerID'] = df['CustomerID'].astype(int)
    9 
   10     # 3. ã‚­ãƒ£ãƒ³ã‚»ãƒ«ãƒ»è¿”å“ã®é™¤å¤– (Quantity > 0)
   11     # æ³¨: ã‚­ãƒ£ãƒ³ã‚»ãƒ«åˆ†ã‚’å£²ä¸Šã‹ã‚‰ãƒã‚¤ãƒŠã‚¹ã—ãŸã„å ´åˆã¯æ®‹ã—ã¾ã™ãŒã€
   12     # ä»Šå›ã¯ã€Œè³¼å…¥è¡Œå‹•ã€ã«ãƒ•ã‚©ãƒ¼ã‚«ã‚¹ã™ã‚‹ãŸã‚é™¤å¤–ã—ã¾ã™ã€‚
   13     df = df[df['Quantity'] > 0]
   14 
   15     # 4. ç‰¹æ®ŠãªStockCodeã®é™¤å¤– (æ•°å­—ã§å§‹ã¾ã‚‰ãªã„ã‚‚ã®ã¯é™¤å¤–ãªã©)
   16     # POST(é€æ–™)ãªã©ã¯é™¤å¤–ã™ã‚‹ã®ãŒç„¡é›£ã§ã™
   17     # ç°¡æ˜“çš„ã«ã¯ã€StockCodeãŒæ•°å­—ã§å§‹ã¾ã‚‹ã‚‚ã®ã ã‘æ®‹ã™ã€ã‚ã‚‹ã„ã¯ç‰¹å®šã®ãƒªã‚¹ãƒˆã‚’é™¤å¤–ã™ã‚‹
   18     # ã“ã“ã§ã¯ 'POST', 'D', 'M', 'C2', 'DOT', 'BANK CHARGES' ãªã©ã‚’é™¤å¤–
   19     special_codes = ['POST', 'D', 'M', 'C2', 'DOT', 'BANK CHARGES', 'CRUK']
   20     df = df[~df['StockCode'].isin(special_codes)]
   21 
   22     # 5. UnitPrice > 0 ã®ãƒ‡ãƒ¼ã‚¿ã®ã¿åˆ©ç”¨
   23     df = df[df['UnitPrice'] > 0]
   24 
   25     # ... (ä»¥ä¸‹ã€æ—¥ä»˜å¤‰æ›ãªã©) ...

  ã“ã®å‰å‡¦ç†ã‚’è¿½åŠ ã—ã¦ã‚³ãƒ¼ãƒ‰ã‚’ä¿®æ­£ã—ã¾ã™ã‹ï¼Ÿ
```
:::

ä»•æ§˜ãŒæ›–æ˜§ãªéƒ¨åˆ†ã¾ã§å®Ÿè£…æ–¹é‡ã‚’æ±ºã‚ã¦ã—ã¾ã£ã¦ã„ã‚‹éƒ¨åˆ†ã‚‚ã‚ã‚Šã¾ã™ãŒã€ã“ã“ã§ã¯ LLM ã®è¨€ã†ã“ã¨ã‚’ä¿¡ã˜ã¦ãã®ã¾ã¾å®Ÿè£…ã«ç§»ã‚Šã¾ã—ãŸã€‚

### ç²¾åº¦å‘ä¸Šã®ç›¸è«‡

æ¬¡ã«ã€ãƒ¢ãƒ‡ãƒ«ã®ç²¾åº¦å‘ä¸Šã«ã¤ã„ã¦ç›¸è«‡ã—ã¾ã™ã€‚

```python
æ±åŒ–æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã«ã©ã‚“ãªæ”¹å–„ãŒå¿…è¦ã§ã™ã‹
```

:::details è¿”ç­”
```
âœ¦ æ±åŒ–æ€§èƒ½ï¼ˆæœªçŸ¥ã®ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹äºˆæ¸¬ç²¾åº¦ï¼‰ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã«ã¯ã€ãƒ¢ãƒ‡ãƒ«ãŒãƒ‡ãƒ¼ã‚¿ã®ã€ŒèƒŒå¾Œã«ã‚ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã€ã‚’ã‚ˆã‚Š
  è‰¯ãæ‰ãˆã‚‰ã‚Œã‚‹ã‚ˆã†ã«ã—ã€ã‹ã¤ã€Œå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¸ã®éå‰°é©åˆã€ã‚’é˜²ãå¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

  ç¾åœ¨ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ï¼ˆéå»30æ—¥é–“ã®RFMã®ã¿ï¼‰ã«å¯¾ã—ã¦ã€ä»¥ä¸‹ã®æ”¹å–„ç­–ãŒè€ƒãˆã‚‰ã‚Œã¾ã™ã€‚

  1. ç‰¹å¾´é‡ã®æ‹¡å…… (Feature Engineering)
  ç¾åœ¨ Recency, Frequency, Monetary (éå»30æ—¥) ã®ã¿ã§ã™ãŒã€é¡§å®¢ã®è¡Œå‹•ã‚’ã‚ˆã‚Šå¤šè§’çš„ã«æ‰ãˆã‚‹ç‰¹å¾´é‡ã‚’è¿½åŠ ã—ã¾ã™ã€‚
   * é•·æœŸçš„ãªè¡Œå‹•å±¥æ­´:
     éå»30æ—¥ã ã‘ã§ãªãã€90æ—¥ã‚„180æ—¥ã€ã‚ã‚‹ã„ã¯å…¨æœŸé–“ã®å±¥æ­´ã‚’é›†è¨ˆã—ã¾ã™ã€‚ã€Œæœ€è¿‘30æ—¥ã¯è²·ã£ã¦ã„ãªã„ãŒã€ãã®å‰ã®
     åŠå¹´ã¯é »ç¹ã«è²·ã£ã¦ã„ãŸã€ã¨ã„ã£ãŸé¡§å®¢ã‚’æ‰ãˆã‚‹ãŸã‚ã§ã™ã€‚
   * ãƒˆãƒ¬ãƒ³ãƒ‰: (ç›´è¿‘30æ—¥ã®è³¼å…¥é¡) / (éå»90æ—¥ã®è³¼å…¥é¡å¹³å‡)
     ã®ã‚ˆã†ãªæ¯”ç‡ã‚’è¨ˆç®—ã—ã€è³¼å…¥ãŒå¢—ãˆã¦ã„ã‚‹ã‹æ¸›ã£ã¦ã„ã‚‹ã‹ã®å‚¾å‘ï¼ˆTrendï¼‰ã‚’åŠ ãˆã¾ã™ã€‚
   * å•†å“ã®å¤šæ§˜æ€§: è³¼å…¥ã—ãŸå•†å“ï¼ˆStockCodeï¼‰ã®ç¨®é¡æ•°ï¼ˆãƒ¦ãƒ‹ãƒ¼ã‚¯æ•°ï¼‰ã€‚
   * é¡§å®¢æœŸé–“ (Tenure): åˆå›è³¼å…¥æ—¥ã‹ã‚‰ç¾åœ¨ã¾ã§ã®æ—¥æ•°ã€‚å¤å‚å®¢ã‹æ–°è¦å®¢ã‹ã‚’åŒºåˆ¥ã—ã¾ã™ã€‚
   * å¹³å‡å˜ä¾¡ãƒ»ãƒã‚¹ã‚±ãƒƒãƒˆã‚µã‚¤ã‚º: 1å›ã‚ãŸã‚Šã®å¹³å‡è³¼å…¥é¡ã‚„ã€å¹³å‡è³¼å…¥ç‚¹æ•°ã€‚

  2. ç›®çš„å¤‰æ•°ã®å¤‰æ› (Log Transformation)
  å£²ä¸Šãƒ‡ãƒ¼ã‚¿ã¯ã€Œã¹ãä¹—å‰‡ã€ã«å¾“ã†ã“ã¨ãŒå¤šãã€å°‘æ•°ã®ã€Œè¶…å„ªè‰¯é¡§å®¢ï¼ˆå¤–ã‚Œå€¤ï¼‰ã€ãŒå…¨ä½“ã®èª¤å·®(RMSE)ã‚’å¤§ããæ­ªã‚ã¾ã™
  ã€‚
   * ç›®çš„å¤‰æ•°ï¼ˆè³¼å…¥é‡‘é¡ï¼‰ã‚’ log(x + 1) ã§å¯¾æ•°å¤‰æ›ã—ã¦å­¦ç¿’ã•ã›ã€äºˆæ¸¬å¾Œã« exp(x) - 1
     ã§æˆ»ã™ã“ã¨ã§ã€å¤–ã‚Œå€¤ã®å½±éŸ¿ã‚’æŠ‘ãˆã€ãƒ¢ãƒ‡ãƒ«ã®å®‰å®šæ€§ã‚’é«˜ã‚ã¾ã™ã€‚

  3. ãƒ¢ãƒ‡ãƒ«ã®å¤‰æ›´
   * RandomForest ã‚‚å¼·åŠ›ã§ã™ãŒã€ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ‡ãƒ¼ã‚¿ã§ã¯ Gradient Boosting Decision Trees (GBDT) (ä¾‹:
     scikit-learnã® HistGradientBoostingRegressor, XGBoost, LightGBMãªã©)
     ãŒã‚ˆã‚Šé«˜ã„ç²¾åº¦ã‚’å‡ºã—ã‚„ã™ã„å‚¾å‘ãŒã‚ã‚Šã¾ã™ã€‚

  4. ã‚¼ãƒ­éå‰°ã¸ã®å¯¾ç­– (Zero-Inflation)
   * æ¬¡ã®30æ—¥é–“ã§ã€Œè³¼å…¥ã—ãªã„ï¼ˆ0å††ï¼‰ã€é¡§å®¢ãŒå¤šæ•°ã‚’å ã‚ã¾ã™ã€‚
       * 2æ®µéšãƒ¢ãƒ‡ãƒ«:
         ã€Œè²·ã†ã‹è²·ã‚ãªã„ã‹(åˆ†é¡)ã€Ã—ã€Œã„ãã‚‰è²·ã†ã‹(å›å¸°)ã€ã«åˆ†ã‘ã¦äºˆæ¸¬ã™ã‚‹æ‰‹æ³•ã‚‚ã‚ã‚Šã¾ã™ãŒã€ã¾ãšã¯å¯¾æ•°å¤‰æ›ã¨G
         BDTã§å¯¾å¿œã™ã‚‹ã®ãŒæ‰‹è»½ã§åŠ¹æœçš„ã§ã™ã€‚

  ---

  ã“ã‚Œã‚‰ã®ä¸­ã§åŠ¹æœãŒé«˜ãã€ã™ãã«å®Ÿè£…å¯èƒ½ãªã€Œç‰¹å¾´é‡ã®è¿½åŠ ï¼ˆæœŸé–“æ‹¡å¼µãƒ»Tenureï¼‰ã€ã¨ã€Œå¯¾æ•°å¤‰æ›ã€ã‚’å®Ÿè£…ã—ã¦åŠ¹æœã‚’
  æ¤œè¨¼ã—ã¾ã—ã‚‡ã†ã‹ï¼Ÿ
```
:::

ã“ã®å›ç­”ã‚’ã‚‚ã¨ã«ã€ã¾ãšã¯ç‰¹å¾´é‡è¿½åŠ ã¨å¯¾æ•°å¤‰æ›ã‚’å®Ÿè£…ã—ã¦ã‚‚ã‚‰ã„ã¾ã—ãŸã€‚ã“ã‚Œã«ã‚ˆã‚Šã€MAEãŒæ”¹å–„ã—ã¾ã—ãŸã€‚
ãŸã æœ¬æ¥ã¯ã€æ§˜ã€…ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®ç²¾åº¦å½±éŸ¿ã‚’ä¸€è²«æ€§ã‚’æŒã£ã¦è©•ä¾¡ã™ã‚‹ãŸã‚ã«ã€ç›®çš„ã«å¿œã˜ã¦æ¤œè¨¼æ–¹æ³•ã‚’ãã¡ã‚“ã¨å®šç¾©ã—ï¼ˆä¾‹ãˆã°äº¤å·®æ¤œè¨¼ã‚’ï¼‰å®Ÿè£…ã™ã‚‹ã“ã¨ãŒæœ€å„ªå…ˆã«ãªã‚Šã¾ã™ã€‚
ç¶šã„ã¦ã€Gemini ã‹ã‚‰ã®ææ¡ˆé€šã‚Šã€GBDT ã¸ã®å¤‰æ›´ã€ãã—ã¦ãƒã‚¤ãƒ‘ãƒ©ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¨ã‚¼ãƒ­éå‰°ãƒ¢ãƒ‡ãƒ«(è³¼å…¥æœ‰ç„¡ã®äº‹å‰æ¨å®š)ã®è¿½åŠ ã‚’æŒ‡ç¤ºã—ã¾ã—ãŸã€‚

:::details ç”Ÿæˆã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰ï¼ˆGBDT + Zero-Inflated Modelï¼‰
```python
# /// script
# dependencies = ["pandas", "scikit-learn", "numpy"]
# ///
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor, HistGradientBoostingClassifier
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import mean_squared_error, mean_absolute_error, roc_auc_score
from sklearn.inspection import permutation_importance
import datetime

def load_and_preprocess(filepath):
    print("Loading data...")
    # Use specific encoding as detected in exploration
    df = pd.read_csv(filepath, encoding='ISO-8859-1')
    
    # Preprocessing
    print("Preprocessing...")
    
    # 1. Drop duplicates
    df.drop_duplicates(inplace=True)

    # 2. Drop rows with missing CustomerID and convert to int
    df.dropna(subset=['CustomerID'], inplace=True)
    df['CustomerID'] = df['CustomerID'].astype(int)
    
    # 3. Exclude cancelled orders (Quantity > 0)
    df = df[df['Quantity'] > 0]
    
    # 4. Exclude special StockCodes that are not actual products
    special_codes = ['POST', 'D', 'M', 'C2', 'DOT', 'BANK CHARGES', 'CRUK', 'AMAZONFEE']
    df = df[~df['StockCode'].isin(special_codes)]

    # 5. Exclude rows with zero or negative UnitPrice
    df = df[df['UnitPrice'] > 0]
    
    # Convert InvoiceDate to datetime
    df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])
    
    # Calculate TotalAmount
    df['TotalAmount'] = df['Quantity'] * df['UnitPrice']
    
    return df

def get_features_and_target(df, cutoff_date, known_customers):
    """
    Generates features and target for a specific cutoff date.
    Target: 30 days after cutoff.
    Features:
        - 30 days window
        - 90 days window
        - 180 days window
        - Global: Recency, Tenure
    """
    
    # Initialize result dataframe
    res = pd.DataFrame({'CustomerID': list(known_customers)})
    
    # --- Feature Engineering ---
    
    # 1. Global Features (Recency, Tenure)
    # Filter data strictly BEFORE cutoff
    df_hist = df[df['InvoiceDate'] < cutoff_date]
    
    # Last and First purchase dates
    global_agg = df_hist.groupby('CustomerID')['InvoiceDate'].agg(['max', 'min']).reset_index()
    global_agg.rename(columns={'max': 'last_purchase_date', 'min': 'first_purchase_date'}, inplace=True)
    
    res = res.merge(global_agg, on='CustomerID', how='left')
    
    # Recency: Days since last purchase
    # Tenure: Days since first purchase
    res['recency'] = (cutoff_date - res['last_purchase_date']).dt.days
    res['tenure'] = (cutoff_date - res['first_purchase_date']).dt.days
    
    # Fill missing (inactive customers? technically known_customers should be in df_hist but safe to fill)
    res['recency'] = res['recency'].fillna(365)
    res['tenure'] = res['tenure'].fillna(0)
    
    # Drop date columns
    res = res.drop(columns=['last_purchase_date', 'first_purchase_date'])

    # 2. Windowed Features (30, 90, 180 days)
    windows = [30, 90, 180]
    
    for days in windows:
        start_date = cutoff_date - datetime.timedelta(days=days)
        df_window = df[(df['InvoiceDate'] >= start_date) & (df['InvoiceDate'] < cutoff_date)]
        
        agg_dict = {
            'TotalAmount': 'sum',
            'InvoiceNo': 'nunique',
            'StockCode': 'nunique'
        }
        
        win_agg = df_window.groupby('CustomerID').agg(agg_dict).reset_index()
        
        # Rename with suffix
        suffix = f"{days}d"
        win_agg.rename(columns={
            'TotalAmount': f'monetary{suffix}',
            'InvoiceNo': f'frequency{suffix}',
            'StockCode': f'unique_items{suffix}'
        }, inplace=True)
        
        res = res.merge(win_agg, on='CustomerID', how='left')
        
        # Fill NaNs with 0
        cols = [f'monetary{suffix}', f'frequency{suffix}', f'unique_items{suffix}']
        res[cols] = res[cols].fillna(0)

    # --- Target Generation ---
    target_end = cutoff_date + datetime.timedelta(days=30)
    df_target = df[(df['InvoiceDate'] >= cutoff_date) & (df['InvoiceDate'] < target_end)]
    
    target_agg = df_target.groupby('CustomerID')['TotalAmount'].sum().reset_index()
    target_agg.rename(columns={'TotalAmount': 'target_amount'}, inplace=True)
    
    # Merge target
    res = res.merge(target_agg, on='CustomerID', how='left')
    res['target_amount'] = res['target_amount'].fillna(0)
    
    return res

def main():
    filepath = 'data.csv'
    df = load_and_preprocess(filepath)
    
    print(f"Data range: {df['InvoiceDate'].min()} to {df['InvoiceDate'].max()}")
    
    max_date = df['InvoiceDate'].max()
    
    # Define Test Cutoff (Leave 30 days for target)
    test_cutoff = max_date - datetime.timedelta(days=30)
    test_cutoff = test_cutoff.replace(hour=0, minute=0, second=0, microsecond=0)
    
    print(f"Test Cutoff: {test_cutoff}")
    
    # Define Training Cutoffs (Multiple monthly slices)
    train_cutoffs = []
    current_cutoff = test_cutoff - datetime.timedelta(days=30)
    for _ in range(6): 
        if current_cutoff < df['InvoiceDate'].min() + datetime.timedelta(days=30):
            break
        train_cutoffs.append(current_cutoff)
        current_cutoff -= datetime.timedelta(days=30)
        
    print(f"Training Cutoffs: {train_cutoffs}")
    
    train_dfs = []
    
    for cutoff in train_cutoffs:
        print(f"Generating training data for cutoff: {cutoff}")
        known_mask = df['InvoiceDate'] < cutoff
        known_customers = df.loc[known_mask, 'CustomerID'].unique()
        
        train_chunk = get_features_and_target(df, cutoff, known_customers)
        train_dfs.append(train_chunk)
        
    train_data = pd.concat(train_dfs, ignore_index=True)
    
    print(f"Generating test data for cutoff: {test_cutoff}")
    known_customers_test = df.loc[df['InvoiceDate'] < test_cutoff, 'CustomerID'].unique()
    test_data = get_features_and_target(df, test_cutoff, known_customers_test)
    
    print(f"Train shape: {train_data.shape}")
    print(f"Test shape: {test_data.shape}")
    
    # Features and Target
    features = ['recency', 'tenure']
    for days in [30, 90, 180]:
        features.extend([f'monetary_{days}d', f'frequency_{days}d', f'unique_items_{days}d'])
    
    target = 'target_amount'
    
    X_train = train_data[features]
    y_train = train_data[target]
    
    X_test = test_data[features]
    y_test = test_data[target]

    # --- Two-Stage Model Implementation ---

    # Hyperparameter Search Space
    param_dist = {
        'learning_rate': [0.01, 0.05, 0.1, 0.2],
        'max_iter': [50, 100, 200],
        'max_depth': [3, 5, 10, None],
        'min_samples_leaf': [10, 20, 50],
        'l2_regularization': [0.0, 0.1, 1.0]
    }

    # Stage 1: Classification (Buy or Not)
    print("\n--- Stage 1: Classification Model (Purchase Probability) ---")
    y_train_class = (y_train > 0).astype(int)
    y_test_class = (y_test > 0).astype(int)
    
    print("Tuning Classifier...")
    clf = HistGradientBoostingClassifier(random_state=42)
    clf_search = RandomizedSearchCV(clf, param_dist, n_iter=10, cv=3, scoring='roc_auc', random_state=42, n_jobs=-1)
    clf_search.fit(X_train, y_train_class)
    
    best_clf = clf_search.best_estimator_
    print(f"Best Classifier Params: {clf_search.best_params_}")
    
    # Predict Probabilities
    prob_purchase_test = best_clf.predict_proba(X_test)[:, 1]
    auc = roc_auc_score(y_test_class, prob_purchase_test)
    print(f"Classifier ROC AUC: {auc:.4f}")

    # Stage 2: Regression (Amount | Buy)
    print("\n--- Stage 2: Regression Model (Conditional Amount) ---")
    # Filter training data to only include positive purchasers
    mask_positive = y_train > 0
    X_train_reg = X_train[mask_positive]
    y_train_reg = y_train[mask_positive]
    
    # Log transform target for regression
    y_train_reg_log = np.log1p(y_train_reg)
    
    print("Tuning Regressor...")
    reg = HistGradientBoostingRegressor(random_state=42)
    reg_search = RandomizedSearchCV(reg, param_dist, n_iter=10, cv=3, scoring='neg_mean_absolute_error', random_state=42, n_jobs=-1)
    reg_search.fit(X_train_reg, y_train_reg_log)
    
    best_reg = reg_search.best_estimator_
    print(f"Best Regressor Params: {reg_search.best_params_}")
    
    # Predict Amounts (for all test cases, assuming they buy)
    pred_log_amount_test = best_reg.predict(X_test)
    pred_amount_test = np.expm1(pred_log_amount_test)

    # --- Final Combination ---
    print("\n--- Final Prediction Combination ---")
    # Final Prediction = Probability * Estimated Amount
    final_preds = prob_purchase_test * pred_amount_test
    
    # Evaluation
    rmse = np.sqrt(mean_squared_error(y_test, final_preds))
    mae = mean_absolute_error(y_test, final_preds)
    
    print("--- Evaluation Results (Two-Stage Model) ---")
    print(f"RMSE: {rmse:.4f}")
    print(f"MAE: {mae:.4f}")
    
    # Feature Importance (using the Classifier as it determines the zero-inflation mostly)
    # We could also look at Regressor importance
    print("\nCalculating Feature Importances (Classifier)...")
    result = permutation_importance(best_clf, X_test, y_test_class, n_repeats=5, random_state=42, n_jobs=-1)
    sorted_idx = result.importances_mean.argsort()
    importances = pd.Series(result.importances_mean[sorted_idx], index=np.array(features)[sorted_idx])
    print(importances.sort_values(ascending=False))

if __name__ == "__main__":
    main()
```
:::

çµæœã¨ã—ã¦ã€RMSEãŒå¤§å¹…ã«æ”¹å–„ã—ã¾ã—ãŸï¼ˆ965.4 -> 807.6ï¼‰ã€‚è³¼å…¥é‡‘é¡0å††ã®é¡§å®¢ã‚’åˆ†é¡ãƒ¢ãƒ‡ãƒ«ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã§ããŸåŠ¹æœã¨è€ƒãˆã‚‰ã‚Œã¾ã™ã€‚
ã—ã‹ã—ã“ã®æ–¹æ³•ã¯æ¨è«–ã‚’2å›è¡Œã†å¿…è¦ãŒã‚ã‚Šã€ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãŒè¤‡é›‘ã«ãªã‚‹ãŸã‚æ³¨æ„ãŒå¿…è¦ã§ã™ã€‚ãŸã å½“åˆã®ãƒ«ãƒ¼ãƒ«ã®é€šã‚Šã€LLM ã‚’ä¿¡ã˜ã¦æ¬¡ã«é€²ã¿ã¾ã™ã€‚

### ã•ã‚‰ãªã‚‹æ”¹å–„ã¨ä»•æ§˜ã®æ•´ç†

ã•ã‚‰ãªã‚‹æ”¹å–„æ¡ˆã‚’èã„ãŸã¨ã“ã‚ã€ãƒˆãƒ¬ãƒ³ãƒ‰ç‰¹å¾´é‡ã‚„è³¼è²·é–“éš”çµ±è¨ˆå€¤ã®è¿½åŠ ãŒææ¡ˆã•ã‚Œã¾ã—ãŸã€‚ã—ã‹ã—å‰è¿°ã®ã‚ˆã†ã«æ§˜ã€…ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®æœ‰åŠ¹æ€§ã‚’æ¯”è¼ƒã™ã‚‹ãŸã‚ã«ã€å…ˆã«é©åˆ‡ãªæ¤œè¨¼æ–¹æ³•ã‚’å›ºã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
ã“ã®ã¾ã¾ç¶šã‘ã¦ã‚‚ç¾åœ¨ã® holdout ã®ã¾ã¾ã§ç¶šã‘ã‚‹æ§˜å­ãªã®ã§ã€å½“åˆã®LLMã‚’ä¿¡ã˜ã‚‹ã¨ã„ã†ãƒ«ãƒ¼ãƒ«ã¯å°‘ã—è„‡ã«ç½®ã„ã¦ã€ä¸€åº¦ä¼šè©±ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¦å†é–‹ã—ã¾ã—ãŸã€‚
å¤‰æ›´å†…å®¹ã¨ä»Šå¾Œã®æŒ‡é‡ã‚’æ˜ç¢ºã«ã™ã‚‹ãŸã‚ã€AIã¨ã®å¯¾è©±å†…å®¹ã‚’ä¸€åº¦ä»•æ§˜æ›¸ (`05_spec.txt`) ã¨ã—ã¦ã¾ã¨ã‚ã•ã›ã€ãã‚Œã‚’å…ƒã«å†åº¦ææ¡ˆã‚’æ±‚ã‚ã¾ã—ãŸã€‚

```
ã¾ãšã¯ @05_spec.txt ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚
ã“ã®å†…å®¹ã«å¯¾ã—ã¦ã•ã‚‰ã«æ±åŒ–æ€§èƒ½ã‚’æ”¹å–„ã™ã‚‹ãŸã‚ã«ã©ã®ã‚ˆã†ãªã“ã¨ãŒå¿…è¦ã¨è€ƒãˆã‚‰ã‚Œã¾ã™ã‹ã€‚
å„ªå…ˆåº¦ã¨åˆã‚ã›ã¦æ•™ãˆã¦ãã ã•ã„ã€‚
```

:::details è¿”ç­”
```
âœ¦ 05_spec.txt ã‚’ç¢ºèªã—ã¾ã—ãŸã€‚è³¼å…¥äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã®æ±åŒ–æ€§èƒ½ã‚’æ”¹å–„ã™ã‚‹ãŸã‚ã«ã€ä»¥ä¸‹ã®ç‚¹ãŒè€ƒãˆã‚‰ã‚Œã¾ã™ã€‚å„ªå…ˆåº¦é †ã«èª¬æ˜ã—ã¾ã™ã€‚

  ---

  æ±åŒ–æ€§èƒ½æ”¹å–„ã®ãŸã‚ã®ææ¡ˆ

  å„ªå…ˆåº¦ï¼šé«˜

   1. ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®å¼·åŒ–ï¼ˆæœªå®Ÿè£…ã®æ”¹å–„æ¡ˆã‚’å„ªå…ˆçš„ã«å®Ÿè£…ï¼‰
       * å†…å®¹: ä»•æ§˜æ›¸ã§ã€Œä»Šå¾Œã®æ”¹å–„æ¡ˆã€ã¨ã—ã¦æŒ™ã’ã‚‰ã‚Œã¦ã„ã‚‹ã€Œãƒˆãƒ¬ãƒ³ãƒ‰ç‰¹å¾´é‡ã€ã‚„ã€Œè³¼è²·é–“éš”çµ±è¨ˆã€ã‚’å„ªå…ˆçš„ã«å®Ÿè£…ã—ã¾ã™ã€‚
           * ãƒˆãƒ¬ãƒ³ãƒ‰ç‰¹å¾´é‡: é¡§å®¢ã®ç›´è¿‘ã¨é•·æœŸã®è³¼è²·è¡Œå‹•ã®å¤‰åŒ–ç‡ã‚’æ‰ãˆã‚‹ç‰¹å¾´é‡ï¼ˆä¾‹: éå»30æ—¥é–“ã®å¹³å‡è³¼å…¥é‡‘é¡ /
             éå»180æ—¥é–“ã®å¹³å‡è³¼å…¥é‡‘é¡ï¼‰ã€‚
           * è³¼è²·é–“éš”çµ±è¨ˆ:
             é¡§å®¢ã®è³¼å…¥é–“éš”ï¼ˆå¹³å‡ã€æ¨™æº–åå·®ã€ä¸­å¤®å€¤ãªã©ï¼‰ã‚’ç‰¹å¾´é‡ã¨ã—ã¦è¿½åŠ ã—ã€å®šæœŸè³¼å…¥è€…ãªã©ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è­˜åˆ¥ã—ã¾ã™ã€‚
       * ç†ç”±:
         ã“ã‚Œã‚‰ã®ç‰¹å¾´é‡ã¯é¡§å®¢ã®è³¼è²·è¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ã‚ˆã‚Šè©³ç´°ã«æ‰ãˆã€ãƒ¢ãƒ‡ãƒ«ãŒæœªçŸ¥ã®ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ã‚‚ã€å€‹ã€…ã®é¡§å®¢ã®å°†æ¥ã®è¡Œå‹•
         ã‚’ã‚ˆã‚Šæ­£ç¢ºã«äºˆæ¸¬ã§ãã‚‹ã‚ˆã†ã«ãªã‚‹ãŸã‚ã€æ±åŒ–æ€§èƒ½å‘ä¸Šã«ç›´æ¥çš„ã«å¯„ä¸ã™ã‚‹ã¨è€ƒãˆã‚‰ã‚Œã¾ã™ã€‚

   2. æ™‚ç³»åˆ—äº¤å·®æ¤œè¨¼ (Time Series Cross-Validation) ã®å°å…¥
       * å†…å®¹:
         ç¾åœ¨ã®è©•ä¾¡æ–¹æ³•ãŒå›ºå®šã®ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã§ã‚ã‚‹ãŸã‚ã€ã“ã‚Œã‚’æ™‚ç³»åˆ—äº¤å·®æ¤œè¨¼ã«ç½®ãæ›ãˆã¾ã™ã€‚å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æ™‚
         ç³»åˆ—é †ã«åˆ†å‰²ã—ã€è¤‡æ•°ã®æœŸé–“ã§ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ã‚’è¡Œã†ã“ã¨ã§ã€ç‰¹å®šã®æœŸé–“ã«éå­¦ç¿’ã™ã‚‹ã“ã¨ãªãã€ã‚ˆã‚Šä¿¡é ¼æ€§ã®é«˜ã„æ±åŒ–æ€§èƒ½
         ã®è©•ä¾¡ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚
       * ç†ç”±:
         æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã§ã¯ã€ç‰¹å®šã®æœŸé–“ã®ãƒ‡ãƒ¼ã‚¿ã«ä¾å­˜ã—ãªã„ãƒ­ãƒã‚¹ãƒˆãªäºˆæ¸¬æ€§èƒ½ãŒé‡è¦ã§ã™ã€‚æ™‚ç³»åˆ—äº¤å·®æ¤œè¨¼ã¯ã€ãƒ¢ãƒ‡ãƒ«ãŒæœªæ¥ã®
         ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ã©ã®ç¨‹åº¦å®‰å®šã—ã¦æ©Ÿèƒ½ã™ã‚‹ã‹ã‚’å®¢è¦³çš„ã«è©•ä¾¡ã™ã‚‹ãŸã‚ã«ä¸å¯æ¬ ã§ã™ã€‚

  å„ªå…ˆåº¦ï¼šä¸­

   3. ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢ã®æ”¹å–„ã¨åŠ¹ç‡åŒ–
       * å†…å®¹: RandomizedSearchCV ã«åŠ ãˆã¦ã€ã‚ˆã‚ŠåŠ¹ç‡çš„ãªãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–æ‰‹æ³•ï¼ˆä¾‹: ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ãƒ©ã‚¤ãƒ–ãƒ©ãƒª Optuna
         ã‚„ Hyperopt
         ãªã©ï¼‰ã®å°å…¥ã‚’æ¤œè¨ã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€é™ã‚‰ã‚ŒãŸè¨ˆç®—ãƒªã‚½ãƒ¼ã‚¹ã§ã€ã‚ˆã‚ŠçŸ­æ™‚é–“ã§æœ€é©ãªãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¦‹ã¤ã‘ã€ãƒ¢
         ãƒ‡ãƒ«ã®ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«ã‚’æœ€å¤§é™ã«å¼•ãå‡ºã™ã“ã¨ãŒæœŸå¾…ã•ã‚Œã¾ã™ã€‚
       * ç†ç”±:
         æœ€é©ãªãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ç²¾åº¦ã¨æ±åŒ–æ€§èƒ½ã«å¤§ããå½±éŸ¿ã™ã‚‹ãŸã‚ã€æ¢ç´¢ã®è³ªã‚’é«˜ã‚ã‚‹ã“ã¨ãŒé‡è¦ã§ã™ã€‚

   4. ã‚¨ãƒ©ãƒ¼åˆ†æã¨ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ã«åŸºã¥ãç‰¹å¾´é‡è¿½åŠ 
       * å†…å®¹:
         ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬èª¤å·®ãŒå¤§ãã„é¡§å®¢ã‚„æœŸé–“ã‚’ç‰¹å®šã—ã€ãã®åŸå› ã‚’è©³ç´°ã«åˆ†æã—ã¾ã™ã€‚ãã®çµæœã‹ã‚‰ã€ãƒ“ã‚¸ãƒã‚¹ã«ãŠã‘ã‚‹ãƒ‰ãƒ¡ã‚¤ãƒ³
         çŸ¥è­˜ï¼ˆä¾‹: å­£ç¯€æ€§ã‚¤ãƒ™ãƒ³ãƒˆã€å¤§è¦æ¨¡ã‚»ãƒ¼ãƒ«ã€æ–°è£½å“ç™ºå£²ãªã©ï¼‰ã‚’åæ˜ ã—ãŸæ–°ãŸãªç‰¹å¾´é‡ï¼ˆä¾‹: ã‚«ãƒ†ã‚´ãƒª/NLP
         æƒ…å ±ã€é¡§å®¢ã®ãƒ‡ãƒ¢ã‚°ãƒ©ãƒ•ã‚£ãƒƒã‚¯æƒ…å ±ãªã©ã€åˆ©ç”¨å¯èƒ½ã§ã‚ã‚Œã°ï¼‰ã‚’è¿½åŠ ã—ã¾ã™ã€‚
       * ç†ç”±:
         å˜ç´”ãªãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ–ãƒ³ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã ã‘ã§ãªãã€ãƒ“ã‚¸ãƒã‚¹ã®èƒŒæ™¯ã‚’ç‰¹å¾´é‡ã«è½ã¨ã—è¾¼ã‚€ã“ã¨ã§ã€ã‚ˆã‚Šç¾å®Ÿä¸–ç•Œã®è¤‡é›‘ãªè³¼è²·
         ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒ¢ãƒ‡ãƒ«ãŒå­¦ç¿’ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã€æ±åŒ–æ€§èƒ½ã¨è§£é‡ˆæ€§ã®ä¸¡é¢ã§æ”¹å–„ãŒè¦‹è¾¼ã‚ã¾ã™ã€‚

  å„ªå…ˆåº¦ï¼šä½

   5. ä»£æ›¿ãƒ¢ãƒ‡ãƒ«ãŠã‚ˆã³ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ã®æ¤œè¨
       * å†…å®¹: ç¾åœ¨ã® HistGradientBoosting ãƒ¢ãƒ‡ãƒ«ã«åŠ ãˆã€ä»–ã®å¼·åŠ›ãªå‹¾é…ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ï¼ˆLightGBM, XGBoost
         ãªã©ï¼‰ã‚„ã€ä»•æ§˜æ›¸ã«ã‚‚è¨˜è¼‰ã®ã‚ã‚‹ Tweedieå›å¸°
         ã®ã‚ˆã†ãªä»£æ›¿ãƒ¢ãƒ‡ãƒ«ã‚’è©¦ã—ã¾ã™ã€‚ã•ã‚‰ã«ã€ã“ã‚Œã‚‰ã®è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ã®å°å…¥ã‚‚æ¤œè¨ã—ã¾ã™ã€‚
       * ç†ç”±:
         è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ã¯ã€å˜ä¸€ãƒ¢ãƒ‡ãƒ«ã®å¼±ç‚¹ã‚’è£œå®Œã—ã€äºˆæ¸¬ã®å®‰å®šæ€§ã¨æ±åŒ–æ€§èƒ½ã‚’é«˜ã‚ã‚‹åŠ¹
         æœãŒæœŸå¾…ã§ãã¾ã™ã€‚ç‰¹ã« Tweedieå›å¸° ã¯ Two-Stage
         ãƒ¢ãƒ‡ãƒ«ã®ä»£æ›¿ã¨ã—ã¦ã€å˜ä¸€ãƒ¢ãƒ‡ãƒ«ã§ã‚¼ãƒ­ã¨é‡‘é¡ã®ä¸¡æ–¹ã‚’äºˆæ¸¬ã§ãã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

  ---

  ã“ã‚Œã‚‰ã®æ”¹å–„ç­–ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®ç²¾åº¦å‘ä¸Šã ã‘ã§ãªãã€æœªçŸ¥ã®ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹ãƒ­ãƒã‚¹ãƒˆæ€§ã‚’é«˜ã‚ã€ã‚ˆã‚Šå®Ÿç”¨çš„ãªè³¼å…¥äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã¸ã¨é€²åŒ–
  ã•ã›ã‚‹ãŸã‚ã«é‡è¦ã§ã‚ã‚‹ã¨è€ƒãˆã¾ã™ã€‚
```
:::

ã“ã“ã§ã‚ˆã†ã‚„ãäº¤å·®æ¤œè¨¼ã®ã‚¢ã‚¤ãƒ‡ã‚¢ãŒå‡ºã¦ãã¾ã—ãŸã€‚ã“ã®ã¾ã¾ã€å„ªå…ˆåº¦ãŒé«˜ã„ã¨åˆ¤æ–­ã•ã‚ŒãŸäº¤å·®æ¤œè¨¼ã¨ç‰¹å¾´é‡è¿½åŠ ã‚’å®Ÿè£…ã—ã¾ã™ã€‚

### æœ€çµ‚çš„ãªã‚³ãƒ¼ãƒ‰

äº¤å·®æ¤œè¨¼ã¨è¿½åŠ ç‰¹å¾´é‡ã‚’å®Ÿè£…ã—ãŸæœ€çµ‚çš„ãªã‚³ãƒ¼ãƒ‰ã§ã™ã€‚

:::details æœ€çµ‚çš„ãªã‚³ãƒ¼ãƒ‰
```python
# /// script
# dependencies = ["pandas", "scikit-learn", "numpy"]
# ///
import pandas as pd
import numpy as np
from sklearn.ensemble import HistGradientBoostingRegressor, HistGradientBoostingClassifier
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import mean_squared_error, mean_absolute_error, roc_auc_score
from sklearn.inspection import permutation_importance
import datetime

def load_and_preprocess(filepath):
    print("Loading data...")
    # Use specific encoding as detected in exploration
    df = pd.read_csv(filepath, encoding='ISO-8859-1')
    
    # Preprocessing
    print("Preprocessing...")
    
    # 1. Drop duplicates
    df.drop_duplicates(inplace=True)

    # 2. Drop rows with missing CustomerID and convert to int
    df.dropna(subset=['CustomerID'], inplace=True)
    df['CustomerID'] = df['CustomerID'].astype(int)
    
    # 3. Exclude cancelled orders (Quantity > 0)
    df = df[df['Quantity'] > 0]
    
    # 4. Exclude special StockCodes
    special_codes = ['POST', 'D', 'M', 'C2', 'DOT', 'BANK CHARGES', 'CRUK', 'AMAZONFEE']
    df = df[~df['StockCode'].isin(special_codes)]

    # 5. Exclude rows with zero or negative UnitPrice
    df = df[df['UnitPrice'] > 0]
    
    # Convert InvoiceDate to datetime
    df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])
    
    # Calculate TotalAmount
    df['TotalAmount'] = df['Quantity'] * df['UnitPrice']
    
    return df

def get_features_and_target(df, cutoff_date, known_customers):
    """
    Generates features and target for a specific cutoff date.
    """
    res = pd.DataFrame({'CustomerID': list(known_customers)})
    
    # --- Feature Engineering ---
    
    # Filter history strictly BEFORE cutoff
    df_hist = df[df['InvoiceDate'] < cutoff_date].copy()
    
    # 1. Global Features (Recency, Tenure)
    global_agg = df_hist.groupby('CustomerID')['InvoiceDate'].agg(['max', 'min']).reset_index()
    global_agg.rename(columns={'max': 'last_purchase_date', 'min': 'first_purchase_date'}, inplace=True)
    
    res = res.merge(global_agg, on='CustomerID', how='left')
    
    res['recency'] = (cutoff_date - res['last_purchase_date']).dt.days
    res['tenure'] = (cutoff_date - res['first_purchase_date']).dt.days
    
    res['recency'] = res['recency'].fillna(365)
    res['tenure'] = res['tenure'].fillna(0)
    
    res = res.drop(columns=['last_purchase_date', 'first_purchase_date'])

    # --- NEW: Purchase Interval Statistics ---
    # Calculate intervals between unique InvoiceDates for each customer
    invoice_dates = df_hist[['CustomerID', 'InvoiceDate']].drop_duplicates().sort_values(['CustomerID', 'InvoiceDate'])
    invoice_dates['prev_date'] = invoice_dates.groupby('CustomerID')['InvoiceDate'].shift(1)
    invoice_dates['days_diff'] = (invoice_dates['InvoiceDate'] - invoice_dates['prev_date']).dt.days
    
    interval_stats = invoice_dates.groupby('CustomerID')['days_diff'].agg(['mean', 'std']).reset_index()
    interval_stats.rename(columns={'mean': 'interval_mean', 'std': 'interval_std'}, inplace=True)
    
    res = res.merge(interval_stats, on='CustomerID', how='left')
    
    # Fill NaNs for intervals (e.g., single purchase customers)
    # -1 indicates undefined interval (only 1 purchase)
    res['interval_mean'] = res['interval_mean'].fillna(-1)
    res['interval_std'] = res['interval_std'].fillna(-1)

    # 2. Windowed Features (30, 90, 180 days)
    windows = [30, 90, 180]
    
    for days in windows:
        start_date = cutoff_date - datetime.timedelta(days=days)
        df_window = df[(df['InvoiceDate'] >= start_date) & (df['InvoiceDate'] < cutoff_date)]
        
        agg_dict = {
            'TotalAmount': 'sum',
            'InvoiceNo': 'nunique',
            'StockCode': 'nunique'
        }
        
        win_agg = df_window.groupby('CustomerID').agg(agg_dict).reset_index()
        
        suffix = f"{days}d"
        win_agg.rename(columns={
            'TotalAmount': f'monetary{suffix}',
            'InvoiceNo': f'frequency{suffix}',
            'StockCode': f'unique_items{suffix}'
        }, inplace=True)
        
        res = res.merge(win_agg, on='CustomerID', how='left')
        
        cols = [f'monetary{suffix}', f'frequency{suffix}', f'unique_items{suffix}']
        res[cols] = res[cols].fillna(0)

    # --- NEW: Trend Features ---
    # Ratio of short-term (30d) activity vs long-term (180d) average activity
    # Normalize by days to get daily average rate comparison
    
    # Avoid division by zero by adding small epsilon
    epsilon = 1e-6
    
    # Monetary Trend: (30d sum / 30) / (180d sum / 180)
    res['trend_monetary'] = (res['monetary_30d'] / 30) / ((res['monetary_180d'] / 180) + epsilon)
    
    # Frequency Trend
    res['trend_frequency'] = (res['frequency_30d'] / 30) / ((res['frequency_180d'] / 180) + epsilon)

    # --- Target Generation ---
    target_end = cutoff_date + datetime.timedelta(days=30)
    df_target = df[(df['InvoiceDate'] >= cutoff_date) & (df['InvoiceDate'] < target_end)]
    
    target_agg = df_target.groupby('CustomerID')['TotalAmount'].sum().reset_index()
    target_agg.rename(columns={'TotalAmount': 'target_amount'}, inplace=True)
    
    res = res.merge(target_agg, on='CustomerID', how='left')
    res['target_amount'] = res['target_amount'].fillna(0)
    
    return res

def train_and_evaluate(train_data, test_data, features, target, fold_name="Fold"):
    X_train = train_data[features]
    y_train = train_data[target]
    
    X_test = test_data[features]
    y_test = test_data[target]

    # Hyperparameter Search Space
    param_dist = {
        'learning_rate': [0.01, 0.05, 0.1, 0.2],
        'max_iter': [50, 100, 200],
        'max_depth': [3, 5, 10, None],
        'min_samples_leaf': [10, 20, 50],
        'l2_regularization': [0.0, 0.1, 1.0]
    }

    # Stage 1: Classification (Buy or Not)
    y_train_class = (y_train > 0).astype(int)
    y_test_class = (y_test > 0).astype(int)
    
    clf = HistGradientBoostingClassifier(random_state=42)
    clf_search = RandomizedSearchCV(clf, param_dist, n_iter=10, cv=3, scoring='roc_auc', random_state=42, n_jobs=-1)
    clf_search.fit(X_train, y_train_class)
    
    best_clf = clf_search.best_estimator_
    prob_purchase_test = best_clf.predict_proba(X_test)[:, 1]
    auc = roc_auc_score(y_test_class, prob_purchase_test)

    # Stage 2: Regression (Amount | Buy)
    mask_positive = y_train > 0
    X_train_reg = X_train[mask_positive]
    y_train_reg = y_train[mask_positive]
    
    y_train_reg_log = np.log1p(y_train_reg)
    
    reg = HistGradientBoostingRegressor(random_state=42)
    reg_search = RandomizedSearchCV(reg, param_dist, n_iter=10, cv=3, scoring='neg_mean_absolute_error', random_state=42, n_jobs=-1)
    reg_search.fit(X_train_reg, y_train_reg_log)
    
    best_reg = reg_search.best_estimator_
    
    pred_log_amount_test = best_reg.predict(X_test)
    pred_amount_test = np.expm1(pred_log_amount_test)

    # Final Combination
    final_preds = prob_purchase_test * pred_amount_test
    
    rmse = np.sqrt(mean_squared_error(y_test, final_preds))
    mae = mean_absolute_error(y_test, final_preds)
    
    return {
        'fold': fold_name,
        'rmse': rmse,
        'mae': mae,
        'auc': auc,
        'best_clf_params': clf_search.best_params_,
        'best_reg_params': reg_search.best_params_
    }, best_clf

def main():
    filepath = 'data.csv'
    df = load_and_preprocess(filepath)
    
    print(f"Data range: {df['InvoiceDate'].min()} to {df['InvoiceDate'].max()}")
    
    features = ['recency', 'tenure', 'interval_mean', 'interval_std', 'trend_monetary', 'trend_frequency']
    for days in [30, 90, 180]:
        features.extend([f'monetary_{days}d', f'frequency_{days}d', f'unique_items_{days}d'])
    target = 'target_amount'

    # --- Cross-Validation Loop ---
    n_folds = 3 # Number of time-series splits
    latest_test_cutoff = df['InvoiceDate'].max() - datetime.timedelta(days=30)
    latest_test_cutoff = latest_test_cutoff.replace(hour=0, minute=0, second=0, microsecond=0)
    
    cv_results = []
    last_clf = None
    last_test_data = None
    
    print(f"\nStarting Time-Series Cross-Validation with {n_folds} folds...")
    
    for i in range(n_folds):
        # Shift cutoff back by 30 days for each fold
        test_cutoff = latest_test_cutoff - datetime.timedelta(days=i*30)
        fold_name = f"Fold {i+1} (Test: {test_cutoff.date()})"
        
        # Check if we have enough history (approx 6 months training + buffer)
        min_history_date = test_cutoff - datetime.timedelta(days=30*7)
        if min_history_date < df['InvoiceDate'].min():
            print(f"Skipping {fold_name}: Insufficient historical data.")
            break
            
        print(f"\nProcessing {fold_name}...")
        
        # 1. Generate Training Data (Multiple slices prior to test_cutoff)
        train_cutoffs = []
        current_cutoff = test_cutoff - datetime.timedelta(days=30)
        # Go back 6 months for training samples
        for _ in range(6): 
            if current_cutoff < df['InvoiceDate'].min() + datetime.timedelta(days=30):
                break
            train_cutoffs.append(current_cutoff)
            current_cutoff -= datetime.timedelta(days=30)
            
        train_dfs = []
        for cutoff in train_cutoffs:
            known_mask = df['InvoiceDate'] < cutoff
            known_customers = df.loc[known_mask, 'CustomerID'].unique()
            train_dfs.append(get_features_and_target(df, cutoff, known_customers))
        
        train_data = pd.concat(train_dfs, ignore_index=True)
        
        # 2. Generate Test Data
        known_customers_test = df.loc[df['InvoiceDate'] < test_cutoff, 'CustomerID'].unique()
        test_data = get_features_and_target(df, test_cutoff, known_customers_test)
        
        print(f"  Train samples: {train_data.shape[0]}, Test samples: {test_data.shape[0]}")
        
        # 3. Train and Evaluate
        metrics, clf = train_and_evaluate(train_data, test_data, features, target, fold_name)
        cv_results.append(metrics)
        
        last_clf = clf
        last_test_data = test_data
        
        print(f"  >> RMSE: {metrics['rmse']:.2f} | MAE: {metrics['mae']:.2f} | AUC: {metrics['auc']:.4f}")

    # --- Summary ---
    print("\n=== Cross-Validation Summary ===")
    if cv_results:
        results_df = pd.DataFrame(cv_results)
        cols = ['fold', 'rmse', 'mae', 'auc']
        print(results_df[cols])
        print("\nAverage Metrics:")
        print(results_df[['rmse', 'mae', 'auc']].mean())
        
        # Feature Importance (from the last fold's classifier)
        if last_clf is not None:
            print("\nFeature Importance (Classifier - Last Fold):")
            result = permutation_importance(last_clf, last_test_data[features], (last_test_data[target] > 0).astype(int), n_repeats=5, random_state=42, n_jobs=-1)
            sorted_idx = result.importances_mean.argsort()
            importances = pd.Series(result.importances_mean[sorted_idx], index=np.array(features)[sorted_idx])
            print(importances.sort_values(ascending=False))
    else:
        print("No folds were completed successfully.")

if __name__ == "__main__":
    main()
```
:::

ã“ã®ã‚³ãƒ¼ãƒ‰ã«ã¯ã€å½“åˆã‚„ã‚ŠãŸã‹ã£ãŸã“ã¨ãŒè¦ç´ ãƒ¬ãƒ™ãƒ«ã§ã¯ã»ã¼ä¸€é€šã‚Šå…¥ã£ã¦ã„ã¾ã™ã€‚ç´°ã‹ã„æ”¹å–„ç‚¹ï¼ˆå›å¸°ã®ç‰¹å¾´é‡é‡è¦åº¦è¡¨ç¤ºã€foldæ•°ã®å¢—åŠ ãªã©ï¼‰ã¯æ®‹ã£ã¦ã„ã¾ã™ãŒã€ã“ã“ã¾ã§ã¨ã—ã¾ã™ã€‚
ã“ã®é€²ã‚æ–¹ã§ã¯ã€LLM ã®ææ¡ˆã‚’åŸºæœ¬çš„ã«å—ã‘å…¥ã‚ŒãªãŒã‚‰é€²ã‚ã¾ã—ãŸãŒã€ä¸€è¦‹çš„ç¢ºã«æ€ãˆã‚‹ææ¡ˆã§ã‚‚æœ€ã‚‚é‡è¦ãªæ¤œè¨¼è¨­è¨ˆãƒ»å®Ÿè£…ãŒå¾Œå›ã—ã«ãªã‚‹ã“ã¨ãŒã‚ã‚Šã€æ–¹é‡ãƒ¬ãƒ™ãƒ«ã§å…¨ã¦éµœå‘‘ã¿ã«ã™ã‚‹ã®ã¯å±é™ºã ã¨æ„Ÿã˜ã¾ã—ãŸã€‚


## ãŠã‚ã‚Šã«

ä»Šå›ã®å®Ÿé¨“ã‚’é€šã—ã¦æ„Ÿã˜ãŸã“ã¨ã‚’ã¾ã¨ã‚ã¾ã™ã€‚

**æŒ‡ç¤ºå‡ºã—ã®ãƒãƒ©ãƒ³ã‚¹**
ä¸€éƒ¨èª˜å°çš„ãªä¼šè©±ã‚‚ã‚ã‚Šã¾ã—ãŸãŒã€å½“åˆã®ç›®çš„ã¯æ¦‚ã­é”æˆã§ãã¾ã—ãŸã€‚
ç´°ã‹ã„ä¿®æ­£ã¯è‡ªåˆ†ã§ç›´æ¥ç·¨é›†ã™ã‚‹æ–¹ãŒæ—©ã„ä¸€æ–¹ã€å¤‰æ›´å†…å®¹ã¯æ˜ç¢ºã ãŒã‚³ãƒ¼ãƒ‰é‡ãŒå¤šã„å ´åˆï¼ˆäº¤å·®æ¤œè¨¼ã®å®Ÿè£…ãªã©ï¼‰ã¯ã€AIã«ä»»ã›ã‚‹ã“ã¨ã§åŠ¹ç‡çš„ã«é€²ã‚ã‚‰ã‚Œã‚‹ã¨æ„Ÿã˜ã¾ã—ãŸã€‚

**ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®é‡è¦æ€§ã¨è² æ‹…**
ãƒ¬ãƒ“ãƒ¥ãƒ¼ã¯éå¸¸ã«é‡è¦ã§ã™ãŒã€åŒæ™‚ã«è² æ‹…ã‚‚å¤§ãã„ã¨æ„Ÿã˜ã¾ã—ãŸã€‚
ä»•æ§˜ãŒæ›–æ˜§ãªéƒ¨åˆ†ã‚’LLMãŒè£œå®Œã—ã¦ãã‚Œã‚‹ã“ã¨ã‚‚ã‚ã‚Šã¾ã™ãŒã€å¿…ãšã—ã‚‚æ„å›³é€šã‚Šã¨ã¯é™ã‚‰ãªã„ãŸã‚ã€å…¥å¿µãªãƒã‚§ãƒƒã‚¯ãŒæ¬ ã‹ã›ã¾ã›ã‚“ã€‚
ã¾ãŸã€ä¼šè©±ã‚’é‡ã­ã‚‹ã”ã¨ã«å¤‰æ›´ç®‡æ‰€ã®ç‰¹å®šã‚„ãƒ¬ãƒ“ãƒ¥ãƒ¼ä½œæ¥­è‡ªä½“ãŒè¾›ããªã‚Šã€å¿ƒç†çš„ãªè² æ‹…ãŒå¢—ã—ã¾ã—ãŸã€‚å¤‰æ›´ç®‡æ‰€ã®ç‰¹å®šã¯ãƒ„ãƒ¼ãƒ«ã®UIæ”¹å–„ã§ç·©å’Œã•ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ãŒã€ãƒ¬ãƒ“ãƒ¥ãƒ¼è‡ªä½“ã®è² æ‹…ã«ã¤ã„ã¦ã¯ã€æ…£ã‚Œã‚„é‹ç”¨ä¸Šã®å·¥å¤«ãŒå¿…è¦ãã†ã§ã™ã€‚

**ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ç®¡ç†**
ä¼šè©±å½¢å¼ã§ä¿®æ­£ã‚’ç¹°ã‚Šè¿”ã™ã¨ã€å¾ã€…ã«LLMã®ææ¡ˆã®å¹…ãŒç‹­ã¾ã‚‹å‚¾å‘ãŒè¦‹ã‚‰ã‚Œã¾ã—ãŸã€‚é©åº¦ãªã‚¿ã‚¤ãƒŸãƒ³ã‚°ã§è¦ä»¶ã‚„ä»•æ§˜ã‚’æ•´ç†ãƒ»ä¿å­˜ã—ã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒªã‚»ãƒƒãƒˆã™ã‚‹ã“ã¨ãŒæœ‰åŠ¹ã ã¨è€ƒãˆã‚‰ã‚Œã¾ã™ã€‚

**å…¨ä½“çš„ãªæ„Ÿæƒ³**
ç”Ÿæˆã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰ãŒå…¨ãå‹•ã‹ãªã„ã“ã¨ã¯ã»ã¼ãªãã€ãã®ç‚¹ã§ã®ä½“é¨“ã¯éå¸¸ã«è‰¯å¥½ã§ã—ãŸã€‚
ã—ã‹ã—ã€ä¼šè©±ã‚’é€šã˜ã¦å¾ã€…ã«ã‚³ãƒ¼ãƒ‰ã‚’æ”¹å–„ã—ã¦ã„ãã‚¹ã‚¿ã‚¤ãƒ«ã¯ã€ç­†è€…ã«ã¨ã£ã¦ã¯~~ã‹ãªã‚Šç–²ã‚Œã¾ã—ãŸ~~è² æ‹…ãŒå¤§ããæ„Ÿã˜ã‚‰ã‚Œã¾ã—ãŸã€‚ã‚„ã‚ŠãŸã„ã“ã¨ãŒæ˜ç¢ºãªå ´åˆã¯ã€å‰è¨˜äº‹ã®ã‚ˆã†ã«æœ€åˆã«è©³ç´°ãªè¦ä»¶å®šç¾©ã‚’è¡Œã„ã€ä¸€æ‹¬ã§ç”Ÿæˆã—ã¦ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚„ä¼šè©±ã®å›æ•°ã‚’æ¸›ã‚‰ã™ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®æ–¹ãŒã€åŠ¹ç‡çš„ã§åˆã£ã¦ã„ã‚‹ã¨æ„Ÿã˜ã¾ã—ãŸã€‚

ä»Šå¾Œã‚‚ãƒ„ãƒ¼ãƒ«ã®é€²åŒ–ã«åˆã‚ã›ã¦ã€æœ€é©ãªä½¿ã„æ–¹ã‚’æ¢ã£ã¦ã„ããŸã„ã¨æ€ã„ã¾ã™ã€‚
